import{_ as e,o as a,c as o,Q as l}from"./chunks/framework.4636910e.js";const g=JSON.parse('{"title":"llama.cpp","description":"","frontmatter":{"order":2},"headers":[],"relativePath":"docs/latest/models/llama.cpp.md","filePath":"docs/latest/models/llama.cpp.md"}'),t={name:"docs/latest/models/llama.cpp.md"},n=l(`<h1 id="llama-cpp" tabindex="-1">llama.cpp <a class="header-anchor" href="#llama-cpp" aria-label="Permalink to &quot;llama.cpp&quot;">​</a></h1><p><a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noreferrer">llama.cpp</a> is also supported as an LMQL inference backend. This allows the use of models packaged as <code>.gguf</code> files, which run efficiently in CPU-only and mixed CPU/GPU environments using the llama.cpp C++ implementation.</p><h2 id="prerequisites" tabindex="-1">Prerequisites <a class="header-anchor" href="#prerequisites" aria-label="Permalink to &quot;Prerequisites&quot;">​</a></h2><p>Before using llama.cpp models, make sure you have installed its Python bindings via <code>pip install llama-cpp-python</code> in the same environment as LMQL. You also need the <code>sentencepiece</code> or <code>transformers</code> package installed, for tokenization. For GPU-enabled <code>llama.cpp</code> inference, you need to install the <code>llama-cpp-python</code> package with the appropriate build flags, as described in its <a href="https://github.com/abetlen/llama-cpp-python#installation-with-hardware-acceleration" target="_blank" rel="noreferrer"><code>README.md</code></a> file.</p><h2 id="using-llama-cpp-models" tabindex="-1">Using llama.cpp Models <a class="header-anchor" href="#using-llama-cpp-models" aria-label="Permalink to &quot;Using llama.cpp Models&quot;">​</a></h2><p>Just like <a href="./hf.html">Transformers models</a>, you can load llama.cpp models either locally or via a long-lived <code>lmql serve-model</code> inference server.</p><h4 id="model-server" tabindex="-1">Model Server <a class="header-anchor" href="#model-server" aria-label="Permalink to &quot;Model Server&quot;">​</a></h4><p>To start a llama.cpp model server, use the following command:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="hljs"><code><span class="line">lmql serve-model llama.cpp:&lt;PATH TO WEIGHTS&gt;.gguf
</span></code></pre></div><p>This will launch an <a href="https://github.com/eth-sri/lmql/tree/main/src/lmql/models/lmtp" target="_blank" rel="noreferrer">LMTP inference endpoint</a> on <code>localhost:8080</code>, which can be used in LMQL, using a corresponding <a href="./"><code>lmql.model(...)</code></a> object.</p><h4 id="using-the-llama-cpp-endpoint" tabindex="-1">Using the <code>llama.cpp</code> endpoint <a class="header-anchor" href="#using-the-llama-cpp-endpoint" aria-label="Permalink to &quot;Using the \`llama.cpp\` endpoint&quot;">​</a></h4><p>To access a served <code>llama.cpp</code> model, you can use an <code>lmql.model(...)</code> object with the following client-side configuration:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line">lmql.model(<span class="hljs-string">&quot;llama.cpp:&lt;PATH TO WEIGHTS&gt;.gguf&quot;</span>, tokenizer=<span class="hljs-string">&quot;&lt;tokenizer&gt;&quot;</span>)
</span></code></pre></div><p><strong>Model Path</strong> The client-side <code>lmql.model(...)</code> identifier must always match the exact server-side <code>lmql serve-model</code> GGUF location, even if the path does not exist on the client machine. In this context, it is merely used as a unique identifier for the model.</p><p><strong>Tokenizer</strong> When omitting <code>tokenizer=...</code>, LMQL will use the <code>transformers</code>-based tokenizer for <a href="https://huggingface.co/huggyllama/llama-7b" target="_blank" rel="noreferrer"><code>huggyllama/llama-7b</code></a> by default. This works for Llama and Llama-based fine-tuned models, but must be adapted for others. To find a matching tokenizer for your concrete <code>gguf</code> file, look up the <code>transformers</code> equivalent entry on the <a href="https://huggingface.co" target="_blank" rel="noreferrer">HuggingFace model hub</a>. Alternatively, you can use <a href="https://github.com/google/sentencepiece" target="_blank" rel="noreferrer"><code>sentencepiece</code></a> as a tokenization backend. For this, you have to specify the client-side path to a corresponding <code>tokenizer.model</code> file.</p><h4 id="running-without-a-model-server" tabindex="-1">Running Without a Model Server <a class="header-anchor" href="#running-without-a-model-server" aria-label="Permalink to &quot;Running Without a Model Server&quot;">​</a></h4><p>To load the llama.cpp directly as part of the Python process that executes your query program, you can use the <code>local:</code> prefix, followed by the path to the <code>gguf</code> file:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line">lmql.model(<span class="hljs-string">&quot;local:llama.cpp:&lt;PATH TO WEIGHTS&gt;.gguf&quot;</span>, tokenizer=<span class="hljs-string">&quot;&lt;tokenizer&gt;&quot;</span>)
</span></code></pre></div><p>Again, you can omit the <code>tokenizer=...</code> argument if you want to use the default tokenizer for <a href="https://huggingface.co/huggyllama/llama-7b" target="_blank" rel="noreferrer"><code>huggyllama/llama-7b</code></a>. If not, you have to specify a tokenizer, as described above.</p><h2 id="configuring-the-llama-instance" tabindex="-1">Configuring the Llama(...) instance <a class="header-anchor" href="#configuring-the-llama-instance" aria-label="Permalink to &quot;Configuring the Llama(...) instance&quot;">​</a></h2><p>Any parameters passed to <code>lmql serve-model</code> and, when running locally, to <code>lmql.model(...)</code> will be passed to the <code>Llama(...)</code> constructor.</p><p>For example, to configure the <code>Llama(...)</code> instance to use an <code>n_ctx</code> value of 1024, run:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="hljs"><code><span class="line">lmql serve-model llama.cpp:&lt;PATH TO WEIGHTS&gt;.<span class="hljs-built_in">bin</span> --n_ctx <span class="hljs-number">1024</span>
</span></code></pre></div><p>Or, when running locally, you can use <code>lmql.model(&quot;local:llama.cpp:&lt;PATH TO WEIGHTS&gt;.bin&quot;, n_ctx=1024)</code>.</p>`,24),s=[n];function c(r,i,d,p,h,m){return a(),o("div",null,s)}const f=e(t,[["render",c]]);export{g as __pageData,f as default};

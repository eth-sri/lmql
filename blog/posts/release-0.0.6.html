<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Releasing the LMQL Caching Layer (v0.0.6) | LMQL</title>
    <meta name="description" content="Language Model Query Language">
    <link rel="preload stylesheet" href="/assets/style.d660084a.css" as="style">
    
    <script type="module" src="/assets/app.7a112693.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.2ed14f66.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/framework.980cae92.js">
    <link rel="modulepreload" href="/assets/chunks/theme.908ddb20.js">
    <link rel="modulepreload" href="/assets/chunks/VersionSwitch.5d3190fc.js">
    <link rel="modulepreload" href="/assets/blog_posts_release-0.0.6.md.2111a972.lean.js">
    <script async src="/promptdown.js"></script>
    <script src="https://static.cloudflareinsights.com/beacon.min.js?token=f7d2a6b1a0624c51ae4dab9a4239b77d&amp;spa=false"></script>
    <script async src="https://buttons.github.io/buttons.js"></script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-9d8abc1e><!--[--><!--]--><!--[--><span tabindex="-1" data-v-c8291ffa></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-c8291ffa> Skip to content </a><!--]--><!----><header class="VPNav" data-v-9d8abc1e data-v-7ad780c2><div class="VPNavBar" data-v-7ad780c2 data-v-f1abbc6e><div class="container" data-v-f1abbc6e><div class="title" data-v-f1abbc6e><div class="VPNavBarTitle" data-v-f1abbc6e data-v-2973dbb4><a class="title" href="/" data-v-2973dbb4><!--[--><!--]--><!--[--><img class="VPImage logo" src="/lmql.svg" alt data-v-ab19afbb><!--]--><!--[-->LMQL<!--]--><!--[--><!--]--></a></div></div><div class="content" data-v-f1abbc6e><div class="curtain" data-v-f1abbc6e></div><div class="content-body" data-v-f1abbc6e><!--[--><!--]--><div class="VPNavBarSearch search" data-v-f1abbc6e><!--[--><!----><div id="docsearch"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg class="DocSearch-Search-Icon" width="20" height="20" viewBox="0 0 20 20" aria-label="search icon"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-f1abbc6e data-v-f732b5d0><span id="main-nav-aria-label" class="visually-hidden" data-v-f732b5d0>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-f732b5d0 data-v-cb318fec><!--[--><span data-v-cb318fec>Home</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink active" href="/blog/" tabindex="0" data-v-f732b5d0 data-v-cb318fec><!--[--><span data-v-cb318fec>Blog</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/research/index.html" tabindex="0" data-v-f732b5d0 data-v-cb318fec><!--[--><span data-v-cb318fec>Research</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/docs/" tabindex="0" data-v-f732b5d0 data-v-cb318fec><!--[--><span data-v-cb318fec>Docs</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/playground/" target="_blank" tabindex="0" data-v-f732b5d0 data-v-cb318fec><!--[--><span data-v-cb318fec>▶ Playground</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-f1abbc6e data-v-283b26e9><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="toggle dark mode" aria-checked="false" data-v-283b26e9 data-v-3329432d data-v-1c29e291><span class="check" data-v-1c29e291><span class="icon" data-v-1c29e291><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-3329432d><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-3329432d><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-f1abbc6e data-v-ef6192dc data-v-e71e869c><!--[--><a class="VPSocialLink no-icon" href="https://github.com/eth-sri/lmql" aria-label="github" target="_blank" rel="noopener" data-v-e71e869c data-v-16cf740a><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><a class="VPSocialLink no-icon" href="https://discord.gg/5djae9gJVB" aria-label="discord" target="_blank" rel="noopener" data-v-e71e869c data-v-16cf740a><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Discord</title><path d="M20.317 4.3698a19.7913 19.7913 0 00-4.8851-1.5152.0741.0741 0 00-.0785.0371c-.211.3753-.4447.8648-.6083 1.2495-1.8447-.2762-3.68-.2762-5.4868 0-.1636-.3933-.4058-.8742-.6177-1.2495a.077.077 0 00-.0785-.037 19.7363 19.7363 0 00-4.8852 1.515.0699.0699 0 00-.0321.0277C.5334 9.0458-.319 13.5799.0992 18.0578a.0824.0824 0 00.0312.0561c2.0528 1.5076 4.0413 2.4228 5.9929 3.0294a.0777.0777 0 00.0842-.0276c.4616-.6304.8731-1.2952 1.226-1.9942a.076.076 0 00-.0416-.1057c-.6528-.2476-1.2743-.5495-1.8722-.8923a.077.077 0 01-.0076-.1277c.1258-.0943.2517-.1923.3718-.2914a.0743.0743 0 01.0776-.0105c3.9278 1.7933 8.18 1.7933 12.0614 0a.0739.0739 0 01.0785.0095c.1202.099.246.1981.3728.2924a.077.077 0 01-.0066.1276 12.2986 12.2986 0 01-1.873.8914.0766.0766 0 00-.0407.1067c.3604.698.7719 1.3628 1.225 1.9932a.076.076 0 00.0842.0286c1.961-.6067 3.9495-1.5219 6.0023-3.0294a.077.077 0 00.0313-.0552c.5004-5.177-.8382-9.6739-3.5485-13.6604a.061.061 0 00-.0312-.0286zM8.02 15.3312c-1.1825 0-2.1569-1.0857-2.1569-2.419 0-1.3332.9555-2.4189 2.157-2.4189 1.2108 0 2.1757 1.0952 2.1568 2.419 0 1.3332-.9555 2.4189-2.1569 2.4189zm7.9748 0c-1.1825 0-2.1569-1.0857-2.1569-2.419 0-1.3332.9554-2.4189 2.1569-2.4189 1.2108 0 2.1757 1.0952 2.1568 2.419 0 1.3332-.946 2.4189-2.1568 2.4189Z"/></svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-f1abbc6e data-v-c8c2ae4b data-v-aa8de344><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-aa8de344><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-aa8de344><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-aa8de344><div class="VPMenu" data-v-aa8de344 data-v-e42ed9b3><!----><!--[--><!--[--><!----><div class="group" data-v-c8c2ae4b><div class="item appearance" data-v-c8c2ae4b><p class="label" data-v-c8c2ae4b>Appearance</p><div class="appearance-action" data-v-c8c2ae4b><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="toggle dark mode" aria-checked="false" data-v-c8c2ae4b data-v-3329432d data-v-1c29e291><span class="check" data-v-1c29e291><span class="icon" data-v-1c29e291><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-3329432d><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-3329432d><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="group" data-v-c8c2ae4b><div class="item social-links" data-v-c8c2ae4b><div class="VPSocialLinks social-links-list" data-v-c8c2ae4b data-v-e71e869c><!--[--><a class="VPSocialLink no-icon" href="https://github.com/eth-sri/lmql" aria-label="github" target="_blank" rel="noopener" data-v-e71e869c data-v-16cf740a><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><a class="VPSocialLink no-icon" href="https://discord.gg/5djae9gJVB" aria-label="discord" target="_blank" rel="noopener" data-v-e71e869c data-v-16cf740a><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Discord</title><path d="M20.317 4.3698a19.7913 19.7913 0 00-4.8851-1.5152.0741.0741 0 00-.0785.0371c-.211.3753-.4447.8648-.6083 1.2495-1.8447-.2762-3.68-.2762-5.4868 0-.1636-.3933-.4058-.8742-.6177-1.2495a.077.077 0 00-.0785-.037 19.7363 19.7363 0 00-4.8852 1.515.0699.0699 0 00-.0321.0277C.5334 9.0458-.319 13.5799.0992 18.0578a.0824.0824 0 00.0312.0561c2.0528 1.5076 4.0413 2.4228 5.9929 3.0294a.0777.0777 0 00.0842-.0276c.4616-.6304.8731-1.2952 1.226-1.9942a.076.076 0 00-.0416-.1057c-.6528-.2476-1.2743-.5495-1.8722-.8923a.077.077 0 01-.0076-.1277c.1258-.0943.2517-.1923.3718-.2914a.0743.0743 0 01.0776-.0105c3.9278 1.7933 8.18 1.7933 12.0614 0a.0739.0739 0 01.0785.0095c.1202.099.246.1981.3728.2924a.077.077 0 01-.0066.1276 12.2986 12.2986 0 01-1.873.8914.0766.0766 0 00-.0407.1067c.3604.698.7719 1.3628 1.225 1.9932a.076.076 0 00.0842.0286c1.961-.6067 3.9495-1.5219 6.0023-3.0294a.077.077 0 00.0313-.0552c.5004-5.177-.8382-9.6739-3.5485-13.6604a.061.061 0 00-.0312-.0286zM8.02 15.3312c-1.1825 0-2.1569-1.0857-2.1569-2.419 0-1.3332.9555-2.4189 2.157-2.4189 1.2108 0 2.1757 1.0952 2.1568 2.419 0 1.3332-.9555 2.4189-2.1569 2.4189zm7.9748 0c-1.1825 0-2.1569-1.0857-2.1569-2.419 0-1.3332.9554-2.4189 2.1569-2.4189 1.2108 0 2.1757 1.0952 2.1568 2.419 0 1.3332-.946 2.4189-2.1568 2.4189Z"/></svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-f1abbc6e data-v-6bee1efd><span class="container" data-v-6bee1efd><span class="top" data-v-6bee1efd></span><span class="middle" data-v-6bee1efd></span><span class="bottom" data-v-6bee1efd></span></span></button></div></div></div></div><!----></header><div class="VPLocalNav fixed reached-top" data-v-9d8abc1e data-v-9e669cc1><!----><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-9e669cc1 data-v-24251f6f><button data-v-24251f6f>Return to top</button><!----></div></div><!----><div class="VPContent" id="VPContent" data-v-9d8abc1e data-v-3cf691b6><div class="VPDoc has-aside" data-v-3cf691b6 data-v-a3c25e27><!--[--><!--]--><div class="container" data-v-a3c25e27><div class="aside" data-v-a3c25e27><div class="aside-curtain" data-v-a3c25e27></div><div class="aside-container" data-v-a3c25e27><div class="aside-content" data-v-a3c25e27><div class="VPDocAside" data-v-a3c25e27 data-v-cb998dce><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" role="navigation" data-v-cb998dce data-v-3a6c4994><div class="content" data-v-3a6c4994><div class="outline-marker" data-v-3a6c4994></div><div class="outline-title" role="heading" aria-level="2" data-v-3a6c4994>On this page</div><nav aria-labelledby="doc-outline-aria-label" data-v-3a6c4994><span class="visually-hidden" id="doc-outline-aria-label" data-v-3a6c4994> Table of Contents for current page </span><ul class="root" data-v-3a6c4994 data-v-463da30f><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-cb998dce></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-a3c25e27><div class="content-container" data-v-a3c25e27><!--[--><!--]--><!----><main class="main" data-v-a3c25e27><div style="position:relative;" class="vp-doc _blog_posts_release-0_0_6" data-v-a3c25e27><div><h1 id="releasing-the-lmql-caching-layer-v0-0-6" tabindex="-1">Releasing the LMQL Caching Layer (v0.0.6) <a class="header-anchor" href="#releasing-the-lmql-caching-layer-v0-0-6" aria-label="Permalink to &quot;Releasing the LMQL Caching Layer (v0.0.6)&quot;">​</a></h1><p><span class="date">May 1, 2023</span></p><p>Today we are releasing LMQL 0.0.6, the first version of LMQL that integrates the <em>LMQL Caching Layer</em>. The caching layer can drastically reduce token use of LLM interaction, lowering both the cost and latency of running queries. In this blog post, we provide a quick overview of the caching layer and demonstrate how it can reduce token use, latency and the number of requests needed to run queries by up to 80%. We observe improvements across a wide range of different scenarios, including <strong>template-based queries, long-form constraints and tool augmentation.</strong></p><p>You can experiment with LMQL in the browser-based <a href="http://lmql.ai/playground" target="_blank" rel="noreferrer">Playground IDE</a> or install the latest version locally, via <code>pip install lmql</code>.</p><h2 id="caching-layer" tabindex="-1">Caching Layer <a class="header-anchor" href="#caching-layer" aria-label="Permalink to &quot;Caching Layer&quot;">​</a></h2><p>The caching layer is implemented as a <strong>tree-based data structure</strong> that caches all model output including logits, tokens, and metadata, allowing the runtime to more efficiently explore the token space of an LLM, even in the presence of multiple variables, constraints and tool augmentation. The cache can be considered an append-only tree, that is explored during query execution, expanding branches according to query code, constraints and speculative execution.</p><p>To illustrate the effect of a caching layer, we consider the following example scenarios, all of which now run in a fraction of the time and with a fraction of the tokens needed with traditional querying methods.</p><h3 id="template-based-queries" tabindex="-1">Template-Based Queries <a class="header-anchor" href="#template-based-queries" aria-label="Permalink to &quot;Template-Based Queries&quot;">​</a></h3><p>When specifying a prompt template with multiple variables to fill in, an LLM typically needs to be invoked once per variable. For instance, consider the following template that guides an LLM in generating a list of things:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">argmax</span>
    <span class="hljs-string">&quot;A list of things not to forget when going to the sea (not travelling): \n&quot;</span>
    <span class="hljs-string">&quot;- Sunglasses \n&quot;</span>
    <span class="hljs-string">&quot;-<span class="hljs-placeholder">[THING]</span>&quot;</span>
    <span class="hljs-string">&quot;-<span class="hljs-placeholder">[THING]</span>&quot;</span>
    <span class="hljs-string">&quot;-<span class="hljs-placeholder">[THING]</span>&quot;</span>
    <span class="hljs-string">&quot;-<span class="hljs-placeholder">[THING]</span>&quot;</span>
<span class="hljs-keyword">from</span>
    <span class="hljs-string">&#39;openai/text-ada-001&#39;</span>
<span class="hljs-keyword">where</span>
    STOPS_AT(THING, <span class="hljs-string">&quot;\n&quot;</span>)
</span></code></pre></div><p><strong>Without Caching:</strong> Tokens: 390, Requests: 4 | <strong>With Caching Layer:</strong> Tokens: 89 (<span style="color:green;">-77%</span>), Requests: 1 (<span style="color:green;">-75%</span>)</p><p>Here, the LLM typically needs to be invoked 4 times, once per <code>[THING]</code> variable. On each call, this incurs a token and latency cost (both with OpenAI and local models). Separate calls are needed, because our template dictates the <code>-</code> token to be inserted before each <code>[THING]</code>.</p><p>With the caching layer, LMQL can now invoke the LLM only once, and fill in all variables with the resulting tokens, as long as the LLM output already aligns naturally with your template. In case the LLM result of the initial invocation at some point no longer aligns with the template, LMQL will automatically re-invoke the LLM from this point on, guaranteeing an overall consistent result that is already parsed into separate <code>[THING]</code> variables.</p><h3 id="short-circuiting-long-constraints" tabindex="-1">Short-Circuiting Long Constraints <a class="header-anchor" href="#short-circuiting-long-constraints" aria-label="Permalink to &quot;Short-Circuiting Long Constraints&quot;">​</a></h3><p>When you specify long constraints like <code>A in [&quot;ABCDE&quot;, &quot;FGHIJK&quot;]</code>, the LMQL runtime guides the LLM to choose one of the provided options and then continues enforcing the sequence until the chosen values is fully decoded. To illustrate, consider the following query:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">argmax</span>
    <span class="hljs-string">&quot;If we have the choice we choose<span class="hljs-placeholder">[OPTION]</span>&quot;</span>
<span class="hljs-keyword">from</span> 
    <span class="hljs-string">&quot;openai/text-ada-001&quot;</span>
<span class="hljs-keyword">where</span>
    OPTION <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;Option A with a whole lot of extra context&quot;</span>, 
        <span class="hljs-string">&quot;Option B with context&quot;</span>, 
        <span class="hljs-string">&quot;Another Option, also with a lot of additional text&quot;</span>
    ]
</span></code></pre></div><div class="language-promptdown vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">promptdown</span><pre pd-text="If we have the choice we choose [OPTION|Option A with a whole lot of extra context]
" animate="true" __animate="true" animate-speed="50" class="promptdown promptdown-compiled" style="opacity:1;"><p pd-shadow-id="43" text="I" pd-insertion-point="true">If we have the choice we choose <span pd-shadow-id="45" pd-instant="false" text="" class="promptdown-var color-orange"><span pd-shadow-id="46" text="O" class="promptdown-var-name">OPTION</span>Option A with a whole lot of extra context</span>
</p></pre></div><p><strong>Without Caching:</strong> Tokens: 123, Requests: 9 | <strong>With Caching Layer:</strong> Tokens: 25 (<span style="color:green;">-80%</span>), Requests: 2 (<span style="color:green;">-78%</span>)</p><p>Here, after the LLM has produced <code>&quot;Option&quot;</code> and then <code>&quot; A&quot;</code>, LMQL short-circuits further model calls and automatically completes the resulting sequence to <code>&quot;Option A with a whole lot of extra context&quot;</code>. This is possible because once <code>Option A</code> has been predicted, the remaining tokens are fully determined by the constraints.</p><h3 id="tool-augmented-queries" tabindex="-1">Tool-Augmented Queries <a class="header-anchor" href="#tool-augmented-queries" aria-label="Permalink to &quot;Tool-Augmented Queries&quot;">​</a></h3><p>Lastly, we consider tool augmented queries. LLM agents and tool augmentation are very powerful paradigms, that allow LLMs to incorporate external knowledge and reasoning into their predictions. However, this comes at a cost: On each tool invocation, the LLM needs to be re-invoked to continue decoding after the tool output has been inserted. This impacts both the token cost and latency of running queries, as many requests have to be send forth and back between the LLM and the tool.</p><p>As an example, consider the following query that augments an LLM with the ability to use a key-value storage, <a href="http://lmql.ai/playground?snippet=kv" target="_blank" rel="noreferrer">also runnable in the browser-based LMQL Playground</a>.</p><!----><p><strong>Without Caching:</strong> Tokens: 5,162, Requests: 12 | <strong>With Caching Layer:</strong> Tokens: 3,481 (<span style="color:green;">-33%</span>), Requests: 8 (<span style="color:green;">-33%</span>)</p><p>Here, whenever the LLM produces an action relating to our key-value storage, we invoke a tool that handles the storage and return the result (to <code>assign</code> and <code>get</code> stored values). The result of each tool invocation is then inserted into the LLM output, and the LLM is re-invoked to continue decoding.</p><p>We count 10 tool interactions which results in 12 requests if we run without caching. However, using the new caching layer, we can reduce this to 8 requests, even undercutting the number of tool interactions. This is possible because the caching layer will not abort LLM generation, if the LLM already correctly predicts the tool output.</p><p>This scenario demonstrates that the natural ability of LLMs to complete sequences can be leveraged to reduce the number of tool interactions, by relying on speculative execution.</p><h2 id="persisting-the-cache" tabindex="-1">Persisting the Cache <a class="header-anchor" href="#persisting-the-cache" aria-label="Permalink to &quot;Persisting the Cache&quot;">​</a></h2><p>Of course, the in-memory cache of the LMQL runtime can also be persisted to disk, allowing you to reuse the cache tree across multiple queries, automatically reducing token cost and latency. In some cases this can even be used to reduce the number of requests to the LLM to 0, e.g. if the cache already contains the desired result.</p><p>To do so, you can simply specify a <code>cache=&quot;file.tokens&quot;</code> parameter in your query code:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">argmax</span>(cache=<span class="hljs-string">&quot;joke.tokens&quot;</span>)
   <span class="hljs-string">&quot;&quot;&quot;A good dad joke. A indicates the punchline
   Q:<span class="hljs-placeholder">[JOKE]</span>
   A:<span class="hljs-placeholder">[PUNCHLINE]</span>&quot;&quot;&quot;</span>
<span class="hljs-keyword">from</span>
   <span class="hljs-string">&quot;openai/text-davinci-003&quot;</span>
<span class="hljs-keyword">where</span>
   <span class="hljs-built_in">len</span>(JOKE) &lt; <span class="hljs-number">120</span> <span class="hljs-keyword">and</span> 
   STOPS_AT(JOKE, <span class="hljs-string">&quot;?&quot;</span>) <span class="hljs-keyword">and</span> 
   STOPS_AT(PUNCHLINE, <span class="hljs-string">&quot;\n&quot;</span>) <span class="hljs-keyword">and</span> 
   <span class="hljs-built_in">len</span>(PUNCHLINE) &gt; <span class="hljs-number">1</span>
</span></code></pre></div><p>The first successful run of this query will persist the cache to <code>joke.tokens</code>. Subsequent runs will then automatically load the cache from disk, and only invoke the LLM if the cache does not contain a match. This also works for queries whose underlying LLM requests only partially overlap, as the tree-based cache data structure will automatically identify matching subtrees.</p><p><strong>Caching During Query Development</strong>: Persisting the cache can be particularly useful during query development, as it allows you to reuse the cache across multiple runs of the same query. A persistent cache will reduce token cost and latency of your query, even if you slightly change the query between runs.</p><h2 id="caveats-and-disabling-the-cache" tabindex="-1">Caveats and Disabling the Cache <a class="header-anchor" href="#caveats-and-disabling-the-cache" aria-label="Permalink to &quot;Caveats and Disabling the Cache&quot;">​</a></h2><p>You can disable the caching layer by specifying <code>cache=False</code> in your query code. This will cause the LMQL runtime to always invoke the LLM, and never use the cache. This is useful for debugging purposes, or if you want to ensure that the LLM is always invoked.</p><p>Further, as the cache currently is implemented as an append-only data structure, it will grow indefinitely. This may be problematic for long-running applications, as the cache will eventually grow to relatively large sizes. In the future, we plan to implement simple strategies to limit the cache size, such as a least-recently-used eviction policy.</p><h2 id="conclusion" tabindex="-1">Conclusion <a class="header-anchor" href="#conclusion" aria-label="Permalink to &quot;Conclusion&quot;">​</a></h2><p>In this post, we introduced the new caching layer of the LMQL runtime, which allows you to reduce the token cost and latency of your queries by reusing previously generated LLM outputs. We demonstrated how the caching layer can be used to reduce the number of LLM invocations in a variety of scenarios, including long constraints, short-circuiting, and tool-augmented queries. We also showed how the cache can be persisted to disk, allowing you to reuse the cache across multiple queries.</p><p>To learn more about LMQL please also check out our <a href="/docs.html">documentation</a>, or join our <a href="https://discord.gg/2Y3Wz2Q" target="_blank" rel="noreferrer">Discord</a> to chat with us directly. We are looking forward to hearing from you!</p></div></div></main><footer class="VPDocFooter" data-v-a3c25e27 data-v-a2d931e4><!--[--><!--]--><!----><!----></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"blog_posts_release-0.0.5.md\":\"6f3e470e\",\"readme.md\":\"82e2e066\",\"blog_posts_release-0.0.6.3.md\":\"a170b883\",\"blog_posts_release-0.0.6.1.md\":\"120a8d77\",\"blog_posts_release-0.0.6.5.md\":\"a82e82a9\",\"blog_posts_release-0.0.6.md\":\"2111a972\",\"blog_posts_release-0.0.6.6.md\":\"958a76e8\",\"blog_posts_release-0.0.6.4.md\":\"a07e6028\",\"blog_posts_release-0.7.md\":\"c15b0585\",\"blog_index.md\":\"c0f7448f\",\"docs_models_azure.md\":\"167a3758\",\"docs_models_hf.md\":\"00577347\",\"docs_development_dev-setup.md\":\"94c94af5\",\"docs_models_llama.cpp.md\":\"89effe2c\",\"docs_latest_models_openai.md\":\"0ecf2534\",\"docs_models_openai.md\":\"39ade1fd\",\"docs_lib_integrations.md\":\"f23c2c07\",\"docs_language_decorators.md\":\"9a3eb9c2\",\"docs_latest_installation.md\":\"4aae67b7\",\"docs_latest_language_reference.md\":\"26684918\",\"docs_latest_models_replicate.md\":\"4762e61c\",\"docs_lib_chat_internal.md\":\"865e3c38\",\"features__1-types.md\":\"2e12a0c5\",\"docs_lib_chat_overview.md\":\"6114091f\",\"docs_lib_chat_serving.md\":\"36ddd098\",\"features_examples_2-constraining.md\":\"cda9d411\",\"features_examples_1-packing-list.md\":\"6002788c\",\"docs_lib_generations.md\":\"4968526e\",\"features_examples_3-multi-part.md\":\"8bbbe3a5\",\"docs_lib_inference-certificates.md\":\"9a213cfe\",\"docs_language_reference.md\":\"93969594\",\"docs_latest_lib_integrations_langchain.md\":\"42ff678a\",\"features_examples_3.6-python.md\":\"376501ff\",\"features_examples_5-wikipedia.md\":\"0f7e4309\",\"docs_latest_lib_integrations_llama_index.md\":\"4f2fe1b9\",\"features_examples_6-chat.md\":\"cc86c446\",\"docs_language_decoding.md\":\"0469ec27\",\"docs_lib_integrations_llama_index.md\":\"f3803dc1\",\"docs_latest_lib_chat_defend.md\":\"5cbe2338\",\"docs_lib_integrations_pandas.md\":\"f1409ba3\",\"index.md\":\"6c38f46d\",\"docs_lib_output.md\":\"6e90a5e7\",\"docs_latest_lib_chat_internal.md\":\"fc68f234\",\"features_examples_2.5-data-types.md\":\"7dcc82ab\",\"docs_latest_lib_chat_serving.md\":\"c33c8e48\",\"features_examples_3.5-distributions.md\":\"9d7164a2\",\"docs_latest_lib_integrations_pandas.md\":\"9b6d3f34\",\"docs_latest_lib_generations.md\":\"e8df2635\",\"docs_latest_language_tools.md\":\"b9561422\",\"features_3-models.md\":\"eaf28c02\",\"features_2-nested.md\":\"3c4c00f3\",\"docs_models_replicate.md\":\"c1a2236a\",\"docs_development_docker-setup.md\":\"0e3e1b2c\",\"docs_latest_lib_python.md\":\"1c370ca3\",\"docs_models_index.md\":\"7dc07083\",\"docs_index.md\":\"526a0692\",\"docs_installation.md\":\"8739ce92\",\"docs_latest_lib_output.md\":\"4442e9f8\",\"docs_latest_models_hf.md\":\"db4e71b2\",\"docs_language_constraints.md\":\"43fe4757\",\"research_index.md\":\"c26a598e\",\"docs_language_tools.md\":\"9aaa96de\",\"docs_latest_language_overview.md\":\"7387dad8\",\"docs_latest_lib_chat.md\":\"00476b1d\",\"docs_latest_models_index.md\":\"d5054f5a\",\"docs_latest_lib_inference-certificates.md\":\"414275b7\",\"docs_lib_chat.md\":\"f8be0cb3\",\"docs_latest_development_documentation.md\":\"c26b8856\",\"docs_latest_development_dev-setup.md\":\"18f17674\",\"docs_latest_development_docker-setup.md\":\"fa59e371\",\"docs_latest_index.md\":\"45032e16\",\"docs_lib_integrations_langchain.md\":\"82a1a8ce\",\"docs_latest_language_constraints_custom-constraints.md\":\"a673b22d\",\"docs_latest_language_decoding.md\":\"018937f6\",\"docs_latest_language_decorators.md\":\"53bd4b02\",\"docs_latest_language_constraints.md\":\"78575736\",\"docs_development_documentation.md\":\"b9530bbc\",\"docs_language_constraints_custom-constraints.md\":\"f81b18dd\",\"docs_language_overview.md\":\"4e5dda69\",\"docs_latest_lib_chat_overview.md\":\"aa834f5b\",\"docs_latest_models_azure.md\":\"d18f3972\",\"docs_language_scripted-prompting.md\":\"1a472d51\",\"docs_latest_lib_integrations.md\":\"eb75ecfa\",\"features_1-code.md\":\"356f0c36\",\"docs_latest_language_nestedqueries.md\":\"815f2dd9\",\"docs_latest_models_llama.cpp.md\":\"9e418aff\",\"docs_latest_language_scripted-prompting.md\":\"12afea6e\",\"docs_language_nestedqueries.md\":\"3fc05b78\",\"features_examples_4-meta-prompting.md\":\"19152cd4\",\"docs_lib_python.md\":\"bc95434a\",\"docs_lib_chat_defend.md\":\"c253ac92\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"LMQL\",\"description\":\"Language Model Query Language\",\"base\":\"/\",\"head\":[],\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"Home\",\"link\":\"/\"},{\"text\":\"Blog\",\"link\":\"/blog/\",\"activeMatch\":\"^/blog/\"},{\"text\":\"Research\",\"link\":\"/research/index.html\"},{\"text\":\"Docs\",\"link\":\"/docs/\",\"activeMatch\":\"^/docs/\"},{\"text\":\"▶ Playground\",\"link\":\"/playground/\",\"target\":\"_blank\"}],\"search\":{\"provider\":\"algolia\",\"options\":{\"appId\":\"TCP95GVDMD\",\"apiKey\":\"f7b4c9cd024536f8085f160045c3aafe\",\"indexName\":\"lmql\"}},\"logo\":\"/lmql.svg\",\"sidebar\":{\"docs/\":[{\"text\":\"\",\"collapsable\":true,\"collapsed\":false,\"base\":\"/docs/\",\"items\":[{\"text\":\"Getting Started\",\"link\":\"/index.md\"},{\"text\":\"Installation Instructions\",\"link\":\"/installation.md\"}]},{\"text\":\"Language\",\"collapsable\":true,\"collapsed\":false,\"base\":\"/docs/language\",\"items\":[{\"text\":\"Overview\",\"link\":\"/overview.md\"},{\"text\":\"Scripted Prompting\",\"link\":\"/scripted-prompting.md\"},{\"text\":\"Constraints\",\"link\":\"/constraints.md\",\"activeMatch\":\"^/docs/language/constraints/\",\"collapsable\":true,\"collapsed\":true,\"items\":[{\"text\":\"Custom Constraints\",\"link\":\"/constraints/custom-constraints.md\"}]},{\"text\":\"Decoders\",\"link\":\"/decoding.md\"},{\"text\":\"Decorators\",\"link\":\"/decorators.md\"},{\"text\":\"Nested Queries <span class=\\\"badge\\\">NEW</span>\",\"link\":\"/nestedqueries.md\"},{\"text\":\"Tool Augmentation\",\"link\":\"/tools.md\"},{\"text\":\"Language Reference\",\"link\":\"/reference.md\"}]},{\"text\":\"Model Support\",\"collapsable\":true,\"collapsed\":false,\"base\":\"/docs/models\",\"items\":[{\"text\":\"Overview\",\"link\":\"/index.md\"},{\"text\":\"Local Models / Transformers\",\"link\":\"/hf.md\"},{\"text\":\"llama.cpp\",\"link\":\"/llama.cpp.md\"},{\"text\":\"OpenAI\",\"link\":\"/openai.md\"},{\"text\":\"Azure\",\"link\":\"/azure.md\"},{\"text\":\"Replicate\",\"link\":\"/replicate.md\"}]},{\"text\":\"Library\",\"collapsable\":true,\"collapsed\":false,\"base\":\"/docs/lib\",\"items\":[{\"text\":\"Chat\",\"link\":\"/chat.md\",\"activeMatch\":\"^/docs/lib/chat/\",\"collapsable\":true,\"collapsed\":true,\"items\":[{\"text\":\"Your First Chatbot\",\"link\":\"/chat/overview.md\"},{\"text\":\"Serving\",\"link\":\"/chat/serving.md\"},{\"text\":\"Internal Reasoning\",\"link\":\"/chat/internal.md\"},{\"text\":\"Defending Against Prompt Injections\",\"link\":\"/chat/defend.md\"}]},{\"text\":\"Generations API <span class=\\\"badge\\\">NEW</span>\",\"link\":\"/generations.md\"},{\"text\":\"Inference Certificates\",\"link\":\"/inference-certificates.md\"},{\"text\":\"Output Streaming\",\"link\":\"/output.md\"},{\"text\":\"Python Integration\",\"link\":\"/python.md\"},{\"text\":\"Other Integrations\",\"link\":\"/integrations.md\",\"activeMatch\":\"^/docs/lib/integrations/\",\"collapsable\":true,\"collapsed\":true,\"items\":[{\"text\":\"LangChain\",\"link\":\"/integrations/langchain.md\"},{\"text\":\"LlamaIndex\",\"link\":\"/integrations/llama_index.md\"},{\"text\":\"Pandas\",\"link\":\"/integrations/pandas.md\"}]}]},{\"text\":\"Development\",\"collapsable\":true,\"collapsed\":false,\"base\":\"/docs/development\",\"items\":[{\"text\":\"Development Environment\",\"link\":\"/dev-setup.md\"},{\"text\":\"LMQL in Docker\",\"link\":\"/docker-setup.md\"},{\"text\":\"Documentation\",\"link\":\"/documentation.md\"}]}],\"docs/latest/\":[{\"text\":\"\",\"collapsable\":true,\"collapsed\":false,\"base\":\"/docs/latest/\",\"items\":[{\"text\":\"Getting Started\",\"link\":\"/index.md\"},{\"text\":\"Installation Instructions\",\"link\":\"/installation.md\"}]},{\"text\":\"Language\",\"collapsable\":true,\"collapsed\":false,\"base\":\"/docs/latest/language\",\"items\":[{\"text\":\"Overview\",\"link\":\"/overview.md\"},{\"text\":\"Scripted Prompting\",\"link\":\"/scripted-prompting.md\"},{\"text\":\"Constraints\",\"link\":\"/constraints.md\",\"activeMatch\":\"^/docs/latest/language/constraints/\",\"collapsable\":true,\"collapsed\":true,\"items\":[{\"text\":\"Custom Constraints\",\"link\":\"/constraints/custom-constraints.md\"}]},{\"text\":\"Decoders\",\"link\":\"/decoding.md\"},{\"text\":\"Decorators\",\"link\":\"/decorators.md\"},{\"text\":\"Nested Queries <span class=\\\"badge\\\">NEW</span>\",\"link\":\"/nestedqueries.md\"},{\"text\":\"Tool Augmentation\",\"link\":\"/tools.md\"},{\"text\":\"Language Reference\",\"link\":\"/reference.md\"}]},{\"text\":\"Model Support\",\"collapsable\":true,\"collapsed\":false,\"base\":\"/docs/latest/models\",\"items\":[{\"text\":\"Overview\",\"link\":\"/index.md\"},{\"text\":\"Local Models / Transformers\",\"link\":\"/hf.md\"},{\"text\":\"llama.cpp\",\"link\":\"/llama.cpp.md\"},{\"text\":\"OpenAI\",\"link\":\"/openai.md\"},{\"text\":\"Azure\",\"link\":\"/azure.md\"},{\"text\":\"Replicate\",\"link\":\"/replicate.md\"}]},{\"text\":\"Library\",\"collapsable\":true,\"collapsed\":false,\"base\":\"/docs/latest/lib\",\"items\":[{\"text\":\"Chat\",\"link\":\"/chat.md\",\"activeMatch\":\"^/docs/latest/lib/chat/\",\"collapsable\":true,\"collapsed\":true,\"items\":[{\"text\":\"Your First Chatbot\",\"link\":\"/chat/overview.md\"},{\"text\":\"Serving\",\"link\":\"/chat/serving.md\"},{\"text\":\"Internal Reasoning\",\"link\":\"/chat/internal.md\"},{\"text\":\"Defending Against Prompt Injections\",\"link\":\"/chat/defend.md\"}]},{\"text\":\"Generations API <span class=\\\"badge\\\">NEW</span>\",\"link\":\"/generations.md\"},{\"text\":\"Inference Certificates\",\"link\":\"/inference-certificates.md\"},{\"text\":\"Output Streaming\",\"link\":\"/output.md\"},{\"text\":\"Python Integration\",\"link\":\"/python.md\"},{\"text\":\"Other Integrations\",\"link\":\"/integrations.md\",\"activeMatch\":\"^/docs/latest/lib/integrations/\",\"collapsable\":true,\"collapsed\":true,\"items\":[{\"text\":\"LangChain\",\"link\":\"/integrations/langchain.md\"},{\"text\":\"LlamaIndex\",\"link\":\"/integrations/llama_index.md\"},{\"text\":\"Pandas\",\"link\":\"/integrations/pandas.md\"}]}]},{\"text\":\"Development\",\"collapsable\":true,\"collapsed\":false,\"base\":\"/docs/latest/development\",\"items\":[{\"text\":\"Development Environment\",\"link\":\"/dev-setup.md\"},{\"text\":\"LMQL in Docker\",\"link\":\"/docker-setup.md\"},{\"text\":\"Documentation\",\"link\":\"/documentation.md\"}]}]},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/eth-sri/lmql\"},{\"icon\":\"discord\",\"link\":\"https://discord.gg/5djae9gJVB\"}]},\"locales\":{},\"scrollOffset\":90,\"cleanUrls\":false}");</script>
    
  </body>
</html>
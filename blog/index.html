<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LMQL: A query language for programming (large) language models.">
  <meta name="keywords" content="GPT-3, language models, LMQL, programming language, query language, ChatGPT">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"/>
  
  <!-- Primary Meta Tags -->
  <title>LMQL Development Blog</title>
  <meta name="title" content="LMQL Development Blog">
  <meta name="description" content="Regular updates on the LMQL project.">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://lmql.ai/">
  <meta property="og:title" content="LMQL Development Blog">
  <meta property="og:description" content="Regular updates on the LMQL project.">
  <meta property="og:image" content="https://lmql.ai/static/images/lmql-social.png">

  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image">
  <meta property="twitter:url" content="https://lmql.ai/">
  <meta property="twitter:title" content="LMQL Development Blog">
  <meta property="twitter:description" content="Regular updates on the LMQL project.">
  <meta property="twitter:image" content="https://lmql.ai/static/images/lmql-social.png"">
  
  
  <!--  set relative path to be / -->
  <base href="/">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="./static/css/val.gen.css">
  <link rel="stylesheet" href="./static/css/lmql.css">
  <link rel="stylesheet" href="./static/css/highlight.min.css">
  <script src="https://kit.fontawesome.com/06bb68d804.js" crossorigin="anonymous"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="./static/js/highlight.min.js"></script>
  <script src="./static/js/lmql.js"></script>
  <script>
    var openPlaygroundElement = null;

function getPlaygroundUrl(next) {
    const host = window.location.host;
    console.log("next is", next)
    if (next) {
      return "https://next.lmql.ai/playground"
    }
    if (host.includes("lmql.ai")) {
        return "https://lmql.ai/playground";
    } else if (host.startsWith("localhost") || host.startsWith("127.0.0.1")) {
        return "http://localhost:3000/playground";
    } else {
        return "https://lbeurerkellner.github.io/green-gold-dachshund-web/playground";
    }
}

function closePlaygroundSnippet() {
    if (openPlaygroundElement) {
        openPlaygroundElement.innerHTML = openPlaygroundElement.originalHTML;
        openPlaygroundElement.classList.remove('playground');
        openPlaygroundElement.style.height = 'auto';

        // show model output div if it exists
        let next = openPlaygroundElement.nextElementSibling;
        while(next.tagName !== 'DIV' && next) {
            next = next.nextElementSibling;
        }

        if (next.classList.contains('highlight-model-output')) {
            next.style.display = 'block';
        }

            openPlaygroundElement = null;
        }
}

function openPlaygroundSnippet(link, snippet) {
    closePlaygroundSnippet();

    const playground = getPlaygroundUrl(snippet.includes("next-"));
    console.log("playground url: " + playground);

    // this is a that was clicked, replace parent div with iframe (temporarily)
    const container = link.parentElement;
    container.classList.add('playground');
    const iframe = document.createElement('iframe');
    iframe.src = ""
    iframe.src = playground + '?embed=' + snippet + ".json"
    iframe.style.width = '100%';
    iframe.style.height = '100%';
    iframe.style.border = 'none';
    
    const height = Math.max(400, container.clientHeight);
    container.originalHTML = container.innerHTML;
    container.innerHTML = '';
    container.style.height = height + 'px';
    container.appendChild(iframe);

    // hide the model output div if it exists
    let next = container.nextElementSibling;
    while(next.tagName !== 'DIV' && next) {
        next = next.nextElementSibling;
    }

    if (next.classList.contains('highlight-model-output')) {
        next.style.display = 'none';
    }

    openPlaygroundElement = container;
}
  </script>

  <!-- Cloudflare Web Analytics --><script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{"token": "f7d2a6b1a0624c51ae4dab9a4239b77d"}'></script><!-- End Cloudflare Web Analytics -->
</head>
<body>
  <div class="topnav">
    <a href="/">
      <img src="./static/images/lmql-text.png" alt="LMQL logo" class="logo">
    </a>
    <a href="https://discord.gg/7eJP4fcyNT" class="hide-on-small">
      üí¨
      Discord
    </a>
    <a href="blog">
      üìù
      Blog
    </a>
    <a href="https://docs.lmql.ai">
      üìñ
      Docs
    </a>
    <a href="https://github.com/eth-sri/lmql">
      üì¶
      GitHub</a>
  </div>


<section class="section blog startpage" id="release-0.0.6.6">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h5 class="title is-2 has-text-left" style="margin-bottom: 2pt;">
            <a href="blog/release-0.0.6.6.html" class="anchor">
                LMQL 0.0.6.6 Released
            </a>
          </h5>
          <div class="content has-text-justified">
          <div class="authors">
            <div class="is-size-5 publication-authors">
              <span class="author-block">
            <a href="mailto:hello@lmql.ai">LMQL Team</a>
        </span>
            </div>
          </div>
            <div class="date">
                Tue, Jul 25, 2023
            </div>
            <section id="lmql-0-0-6-6-released">

<p>We just released LMQL <em>0.0.6.6</em>. This is a minor update with a couple of smaller fixes and improvements.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">lmql.F</span></code> now supports positional arguments:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span id="1-1"><span class="n">greet</span> <span class="o">=</span> <span class="n">lmql</span><span class="o">.</span><span class="n">F</span><span class="p">(</span><span class="s2">"Greet </span><span class="si">{a}</span><span class="s2"> and </span><span class="si">{b}</span><span class="s2">: [GREETING]"</span><span class="p">)</span>
</span><span id="1-2">
</span><span id="1-3"><span class="c1"># call with positional arguments</span>
</span><span id="1-4"><span class="n">greet</span><span class="p">(</span><span class="s2">"Alice"</span><span class="p">,</span> <span class="s2">"Bob"</span><span class="p">)</span> <span class="c1"># Greet Alice and Bob: Hello!</span>
</span><span id="1-5"><span class="c1"># call with keyword arguments</span>
</span><span id="1-6"><span class="n">greet</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="s2">"Alice"</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="s2">"Bob"</span><span class="p">)</span> <span class="c1"># Greet Alice and Bob: Hello!</span>
</span></pre></div>
</div>
<ul class="simple">
<li><p>We improved the error handling of the <code class="docutils literal notranslate"><span class="pre">llama.cpp</span></code> backend and fixed a bug with model identifier parsing.</p></li>
<li><p>We also fixed a bug with the LMTP scheduler, where CPU load was high even when no tasks were present. Thanks to community member <a class="reference external" href="https://github.com/4onen">@4onen</a> for reporting and fixing this!</p></li>
<li><p>Added backend support for <code class="docutils literal notranslate"><span class="pre">auto_gptq</span></code> quantized models, contributed by community member <a class="reference external" href="https://github.com/meditans">@meditans</a>.</p></li>
<li><p>We fixed an issue where for Azure OpenAI models, a dummy configuration <code class="docutils literal notranslate"><span class="pre">api.env</span></code> was needed. See <a class="reference internal" href="#"><span class="xref myst">this issue</span></a> and our <a class="reference external" href="https://docs.lmql.ai/en/stable/language/azure.html">documentation</a> for details. Thanks to community members Missing and <a class="reference external" href="https://github.com/hooman-bayer">@hooman-bayer</a> for their feedback and contributions to this.</p></li>
</ul>
<blockquote>
<div><p><strong>Versioning Note</strong>: 0.0.6.6 is the last release with two leading zeros. Starting with the next release, LMQL will adopt semantic versioning and use a single leading zero, i.e. 0.6.7.</p>
</div></blockquote>
</section>


          </div>
        </div>
    </div>
  </section>
    
<section class="section blog startpage" id="release-0.0.6.5">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h5 class="title is-2 has-text-left" style="margin-bottom: 2pt;">
            <a href="blog/release-0.0.6.5.html" class="anchor">
                LMQL becomes simpler and adds llama.cpp
            </a>
          </h5>
          <div class="content has-text-justified">
          <div class="authors">
            <div class="is-size-5 publication-authors">
              <span class="author-block">
            <a href="mailto:hello@lmql.ai">LMQL Team</a>
        </span>
            </div>
          </div>
            <div class="date">
                Thu, Jul 13, 2023
            </div>
            <section id="lmql-becomes-simpler-and-adds-llama-cpp">

<p>Today we are releasing LMQL 0.0.6.5. This update contains a major simplification of the LMQL syntax, moving it much closer to standard Python. It also includes a <code class="docutils literal notranslate"><span class="pre">llama.cpp</span></code> based inference backend, several bug fixes and other minor improvements.</p>
<p>You can try the latest version of LMQL in your browser at <a class="reference external" href="https://lmql.ai/playground">lmql.ai/playground</a> or install it via <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">lmql</span></code>.</p>
<section id="one-line-is-all-it-takes">
<h2>One Line Is All It Takes<a class="headerlink" href="#one-line-is-all-it-takes" title="Permalink to this heading">¬∂</a></h2>
<p>Most notably, 0.0.6.5 comes with several simplifications of the core syntax of LMQL. Of course, all changes are backwards compatible, so you can continue to use your existing query code and move to the new version without any changes.</p>
<p>With this, we aim to minimize syntactic overhead, employing sensible defaults to enable more concise programs like the following:</p>
<p></p><div class="highlight lmql"><button href="" onclick="openPlaygroundSnippet(this, 'doc-snippets/blog-release-0-0-6-5-md-simple-syntax')">Open In Playground</button><pre><span></span><span class="s2">"One line is all it takes [CONTINUATION]"</span>
</pre></div>
<div class="highlight-model-output notranslate"><div class="highlight">One line is all it takes <span class="variable val1"><span class="variable-name">CONTINUATION</span> Fallin' in love with me.</span></div></div><p></p>
<p><strong>Sensible Defaults</strong> This is possible because LMQL now automatically assumes <code class="docutils literal notranslate"><span class="pre">argmax</span></code> and <code class="docutils literal notranslate"><span class="pre">openai/text-davinci-003</span></code> as (configurable) default model. If you prefer to use
a different model or custom decoder settings, you can still specify them explicitly, e.g. in the <code class="docutils literal notranslate"><span class="pre">@lmql.query</span></code> decorator function as demonstrated later in this post.</p>
<p>Without any additional configuration, the simple query code above translates to a full LMQL program like this:</p>
<p></p><div class="highlight lmql"><button href="" onclick="openPlaygroundSnippet(this, 'doc-snippets/blog-release-0-0-6-5-md-simple-syntax-default')">Open In Playground</button><pre><span></span><span class="kp">argmax</span> <span class="s2">"One line is all it takes [CONTINUATION]"</span> <span class="kn">from</span> <span class="s2">"openai/text-davinci-003"</span>
</pre></div>
<p></p>
<br>
<section id="inline-constraints">
<h3>Inline Constraints<a class="headerlink" href="#inline-constraints" title="Permalink to this heading">¬∂</a></h3>
<p>LMQL now allows you to specify several inline <code class="docutils literal notranslate"><span class="pre">where</span></code> constraints. This enables constraints that refer to local program variables, which means constraints can now be dependent on previous model outputs.</p>
<p></p><div class="highlight lmql"><button href="" onclick="openPlaygroundSnippet(this, 'doc-snippets/blog-release-0-0-6-5-md-list-with-array')">Open In Playground</button><pre><span></span><span class="s2">"A list of awesome Dua Lipa songs:</span><span class="se">\n</span><span class="s2">"</span>
<span class="n">songs</span> <span class="o">=</span> <span class="p">[]</span>

<p><span class="s2">&quot;- New Rules</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="s2">&quot;-[SONG]</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="kp">where</span> <span class="n">STOPS_BEFORE</span><span class="p">(</span><span class="n">SONG</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">songs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">SONG</span><span class="p">)</span></p>
<p><span class="s2">&quot;Out of these, my favorite is[FAVORITE]&quot;</span> <span class="kp">where</span> <span class="n">FAVORITE</span> <span class="ow">in</span> <span class="n">songs</span>
</pre></div></p>
<div class="highlight-model-output notranslate"><div class="highlight">A list of awesome Dua Lipa songs:‚èé
- New Rules
- <span class="variable val1"><span class="variable-name">SONG</span> Don't Start Now</span>
- <span class="variable val1"><span class="variable-name">SONG</span> IDGAF</span>
- <span class="variable val1"><span class="variable-name">SONG</span> Be the One</span>
- <span class="variable val1"><span class="variable-name">SONG</span> Blow Your Mind (Mwah)</span>
Out of these, my favorite is <span class="variable val2"><span class="variable-name">FAVORITE</span> Don't Start Now</span></div></div><p></p>
<p>Note also how in this example LMQL code now reads much more like standard Python code, without any additional level of indentation.</p>
<br>
</section>
<section id="lmql-query-functions">
<h3><code class="docutils literal notranslate"><span class="pre">@lmql.query</span></code> functions<a class="headerlink" href="#lmql-query-functions" title="Permalink to this heading">¬∂</a></h3>
<p>The overhauled syntax also makes LMQL much  easier on the eyes when used with the <code class="docutils literal notranslate"><span class="pre">@lmql.query</span></code> <a class="reference external" href="https://docs.lmql.ai/en/stable/python/python.html">function decorator in Python</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span id="1-1"><span class="kn">import</span> <span class="nn">lmql</span>
</span><span id="1-2"><span class="kn">import</span> <span class="nn">json</span>
</span><span id="1-3">
</span><span id="1-4"><span class="nd">@lmql</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"openai/text-curie-001"</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</span><span id="1-5"><span class="k">def</span> <span class="nf">summarize</span><span class="p">():</span> <span class="s1">'''lmql</span>
</span><span id="1-6"><span class="s1">    """</span>
</span><span id="1-7"><span class="s1">    Provide a summary of Dua Lipa, the pop icon:</span>
</span><span id="1-8"><span class="s1">    {{</span>
</span><span id="1-9"><span class="s1">      "name": "[STRING_VALUE]",</span>
</span><span id="1-10"><span class="s1">      "chart_position": [INT_VALUE],</span>
</span><span id="1-11"><span class="s1">      "top_songs": [[</span>
</span><span id="1-12"><span class="s1">         "[STRING_VALUE]",</span>
</span><span id="1-13"><span class="s1">         "[STRING_VALUE]"</span>
</span><span id="1-14"><span class="s1">      ]]</span>
</span><span id="1-15"><span class="s1">    }}</span>
</span><span id="1-16"><span class="s1">    """ where STOPS_BEFORE(STRING_VALUE, '"') and INT(INT_VALUE) and len(TOKENS(INT_VALUE)) &lt; 3</span>
</span><span id="1-17"><span class="s1">    </span>
</span><span id="1-18"><span class="s1">    return json.loads(context.prompt.split("pop icon:",1)[1])'''</span>
</span><span id="1-19">
</span><span id="1-20"><span class="nb">print</span><span class="p">(</span><span class="n">summarize</span><span class="p">())</span> <span class="c1"># {'name': 'Dua Lipa', 'chart_position': 3415, 'top_songs': ['New Rules', 'Havana']}</span>
</span></pre></div>
</div>
<br>
</section>
<section id="lmql-f-lambda-functions">
<h3><code class="docutils literal notranslate"><span class="pre">lmql.F</span></code> Lambda Functions<a class="headerlink" href="#lmql-f-lambda-functions" title="Permalink to this heading">¬∂</a></h3>
<p>Based on LMQL‚Äôs new minimal syntax, we introduce a novel and concise way to write LLM-based lambda functions. This offers a lightweight entryway to get started with integrated small LLM-based utilities in your code, without having to write a full LMQL program.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span id="1-1"><span class="kn">import</span> <span class="nn">lmql</span>
</span><span id="1-2">
</span><span id="1-3"><span class="n">summarize</span> <span class="o">=</span> <span class="n">lmql</span><span class="o">.</span><span class="n">F</span><span class="p">(</span><span class="s2">"Summarize the following in a few words: </span><span class="si">{data}</span><span class="s2">: [SUMMARY]"</span><span class="p">)</span>
</span><span id="1-4"><span class="n">main_subject</span> <span class="o">=</span> <span class="n">lmql</span><span class="o">.</span><span class="n">F</span><span class="p">(</span><span class="s2">"What is the main subject (noun) of the following text? </span><span class="si">{data}</span><span class="s2">: [SUBJECT]"</span><span class="p">,</span> 
</span><span id="1-5">                      <span class="s2">"len(TOKENS(SUBJECT)) &lt; 20"</span><span class="p">)</span>
</span><span id="1-6">
</span><span id="1-7"><span class="n">text</span> <span class="o">=</span> <span class="s2">"In LMQL, users can specify high-level, logical constraints ..."</span>
</span><span id="1-8">
</span><span id="1-9"><span class="n">summarize</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">text</span><span class="p">)</span> <span class="c1"># LMQL enables high-level constraints to be enforced during text </span>
</span><span id="1-10">                     <span class="c1"># generation, simplifying multi-part prompting and integration.</span>
</span><span id="1-11"><span class="n">main_subject</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">text</span><span class="p">)</span> <span class="c1"># Language Model Query Language (LMQL)</span>
</span></pre></div>
</div>
<br>
<br>
</section>
</section>
<section id="llama-cpp-inference-backend">
<h2><code class="docutils literal notranslate"><span class="pre">llama.cpp</span></code> Inference Backend<a class="headerlink" href="#llama-cpp-inference-backend" title="Permalink to this heading">¬∂</a></h2>
<p>LMQL now also fully integrates with the excellent <a class="reference external" href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> C++ implementation of a number of Transformer-based language models.</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">llama.cpp</span></code> from LMQL is as simple as specifying it in the <code class="docutils literal notranslate"><span class="pre">from</span></code> clause of a query:</p>
<p></p><div class="highlight lmql"><button href="" onclick="openPlaygroundSnippet(this, 'doc-snippets/blog-release-0-0-6-5-md-llama-cpp-blog')">Open In Playground</button><pre><span></span><span class="kp">argmax</span> <span class="s2">"Say 'this is a test':[RESPONSE]"</span> <span class="kn">from</span> <span class="s2">"llama.cpp:&lt;PATH TO WEIGHTS&gt;.bin"</span>
</pre></div>
<p></p>
<p>We support, both, in-process loading of <code class="docutils literal notranslate"><span class="pre">llama.cpp</span></code>, as well as remote inference via <code class="docutils literal notranslate"><span class="pre">lmql</span> <span class="pre">serve-model</span></code>. To learn more about <code class="docutils literal notranslate"><span class="pre">llama.cpp</span></code> and how to use it with LMQL, check out the corresponding chapter in the LMQL <a class="reference external" href="https://docs.lmql.ai/en/latest/language/llama.cpp.html">documentation</a>.</p>
<br>
</section>
<section id="other-changes">
<h2>Other Changes<a class="headerlink" href="#other-changes" title="Permalink to this heading">¬∂</a></h2>
<ul class="simple">
<li><p>LMQL now includes a <code class="docutils literal notranslate"><span class="pre">random</span></code> model backend, which randomly samples tokens from the GPT-2 vocabulary. This is useful for debugging and testing purposes and can be used for data generation in the context of highly constrained query programs.</p></li>
<li><p>Two caching issues have been fixed, avoiding cache collisions which could lead to repeated model outputs.</p></li>
<li><p>More robust query string parsing, allowing for <a class="reference external" href="https://docs.lmql.ai/en/stable/language/scripted_prompts.html#escaping">robust escaping</a> of special characters <code class="docutils literal notranslate"><span class="pre">[</span></code>, <code class="docutils literal notranslate"><span class="pre">]</span></code>, <code class="docutils literal notranslate"><span class="pre">{</span></code> and <code class="docutils literal notranslate"><span class="pre">}</span></code>.</p></li>
<li><p>Added support for <code class="docutils literal notranslate"><span class="pre">transformers</span></code> based Llama models and the associated (fast) implementation of HF tokenizers.</p></li>
<li><p>Simplified Azure OpenAI support, see the relevant chapter in the <a class="reference external" href="https://docs.lmql.ai/en/stable/language/azure.html">documentation</a>.</p></li>
</ul>
<p>We thank community members <a class="reference external" href="https://github.com/minosvasilias">@minosvasilias</a> and <a class="reference external" href="https://github.com/CircArgs">@CircArgs</a> for their contribution to this release.</p>
</section>
</section>


          </div>
        </div>
    </div>
  </section>
    
<section class="section blog startpage" id="release-0.0.6.4">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h5 class="title is-2 has-text-left" style="margin-bottom: 2pt;">
            <a href="blog/release-0.0.6.4.html" class="anchor">
                Releasing LMQL 0.0.6.4: LMTP, Azure, Synchronous API, and more
            </a>
          </h5>
          <div class="content has-text-justified">
          <div class="authors">
            <div class="is-size-5 publication-authors">
              <span class="author-block">
            <a href="mailto:hello@lmql.ai">LMQL Team</a>
        </span>
            </div>
          </div>
            <div class="date">
                Thu, Jun 8, 2023
            </div>
            <section id="releasing-lmql-0-0-6-4-lmtp-azure-synchronous-api-and-more">

<p>Among many things, this update contains several bug fixes and improvements. The most notable changes are:</p>
<ul>
<li><p><strong>Azure OpenAI support</strong> LMQL now supports OpenAI models that are served via Azure. For more information on how to use Azure models, please see the corresponding chapter in the <a class="reference external" href="https://docs.lmql.ai/en/stable/language/azure.html">documentation</a>. Many thanks to <a class="reference external" href="https://github.com/veqtor">@veqtor</a> for contributing this feature.</p></li>
<li><p><strong>Local Models via the Language Model Transport Protocol</strong> LMQL 0.0.6.4 implements a novel protocol to stream token output from local models, vastly improving performance. In our first benchmarks, we observed a 5-6x speedup for local model inference. For more information on how to use local models, please see the corresponding chapter in the <a class="reference external" href="https://docs.lmql.ai/en/stable/language/hf.html">documentation</a>.</p>
<p>To learn more about the internals of the new streaming protocol, i.e. the language model transport protocol (LMTP), you can find more details in <a class="reference external" href="https://github.com/eth-sri/lmql/blob/main/src/lmql/models/lmtp/README.md">this README file</a>. In the future, we intend to implement more model backends using LMTP, streamlining communication between LMQL and models.</p>
 <div style="text-align:center">
  <img src="https://docs.lmql.ai/en/stable/_images/inference.svg" width="80%">
  <br>
  <i>LMQL's new streaming protocol (LMTP) allows for faster local model inference.</i>
 </div>
</li>
<li><p><strong>Synchronous Python API</strong> Next to an <code class="docutils literal notranslate"><span class="pre">async/await</span></code> based API, LMQL now also provides a synchronous API. This means you no longer need to use <code class="docutils literal notranslate"><span class="pre">asyncio</span></code> to use LMQL from Python.</p>
<p>To use the synchronous API, simply declare <code class="docutils literal notranslate"><span class="pre">@lmql.query</span></code> function without the <code class="docutils literal notranslate"><span class="pre">async</span></code> keyword, e.g.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span id="1-1"><span class="kn">import</span> <span class="nn">lmql</span>
</span><span id="1-2">
</span><span id="1-3"><span class="nd">@lmql</span><span class="o">.</span><span class="n">query</span>
</span><span id="1-4"><span class="k">def</span> <span class="nf">hello</span><span class="p">(</span><span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
</span><span id="1-5"><span class="w">    </span><span class="sd">'''lmql</span>
</span><span id="1-6">    <span class="kp">argmax</span> 
</span><span id="1-7">        <span class="s2">"Hello </span><span class="si">{s}</span><span class="s2"> [RESPONSE]"</span> 
</span><span id="1-8">        <span class="k">return</span> <span class="n">RESPONSE</span>
</span><span id="1-9">    <span class="kn">from</span> 
</span><span id="1-10">        <span class="s2">"chatgpt"</span>
</span><span id="1-11">    <span class="sd">'''</span>
</span><span id="1-12">
</span><span id="1-13"><span class="nb">print</span><span class="p">(</span><span class="n">hello</span><span class="p">(</span><span class="s2">"world"</span><span class="p">))</span> <span class="c1"># ['Hello! How can I assist you today?']</span>
</span></pre></div>
</div>
<p>If you instead want to use <code class="docutils literal notranslate"><span class="pre">lmql.run</span></code> in a synchronous context, you can now use <code class="docutils literal notranslate"><span class="pre">lmql.run_sync</span></code> instead. To learn more about how LMQL can be used from Python, check out our <a class="reference external" href="https://docs.lmql.ai/en/stable/python/python.html">documentation</a>.</p>
</li>
<li><p><strong>Improved Tokenizer Backends</strong> LMQL can now use the excellent <a class="reference external" href="https://github.com/openai/tiktoken"><code class="docutils literal notranslate"><span class="pre">tiktoken</span></code> tokenizer</a> as tokenization backend (for OpenAI models). Furthermore, all tokenization backends have been ported to operate on a byte-level, which improves support for multibyte characters and emojis. This is especially relevant for non-English languages and special characters.</p></li>
<li><p><strong>Docker Image</strong> LMQL now provides a Docker image that can be used to run the LMQL playground in a containerized environment. For more information, please see the <a class="reference external" href="https://docs.lmql.ai/en/stable/docker-setup.html">documentation</a>. Many thanks to <a class="reference external" href="https://github.com/SilacciA">@SilacciA</a> for contributing this feature.</p></li>
<li><p><strong>Faster Startup Time</strong> We optimized LMQL‚Äôs import hierarchy, which results in faster module loading time.</p></li>
</ul>
</section>


          </div>
        </div>
    </div>
  </section>
    
<section class="section blog startpage" id="release-0.0.6.3">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h5 class="title is-2 has-text-left" style="margin-bottom: 2pt;">
            <a href="blog/release-0.0.6.3.html" class="anchor">
                LMQL v0.0.6.3
            </a>
          </h5>
          <div class="content has-text-justified">
          <div class="authors">
            <div class="is-size-5 publication-authors">
              <span class="author-block">
            <a href="mailto:hello@lmql.ai">LMQL Team</a>
        </span>
            </div>
          </div>
            <div class="date">
                Thu, May 11, 2023
            </div>
            <section id="lmql-v0-0-6-3">

<p>Today, we are releasing LMQL v0.0.6.3. This update contains several bug fixes and improvements. The most notable changes are:</p>
<ul>
<li><p><strong>Lighter Runtime</strong> As part of our continued efforts, we made LMQL much lighter (no more mandatory <code class="docutils literal notranslate"><span class="pre">transformers</span></code> dependency). By default LMQL now no longer requires <code class="docutils literal notranslate"><span class="pre">transformers</span></code> or PyTorch. If you rely on local models, just install LMQL via <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">lmql[hf]</span></code> to get full Transformers integration.</p></li>
<li><p><strong>Token Constraints</strong> A new function <code class="docutils literal notranslate"><span class="pre">TOKENS(...)</span></code> was added to the LMQL constraint language, allowing you to specify lower and upper bounds or the exact number of tokens to generate for a given variable.</p>
<p></p><div class="highlight lmql"><button href="" onclick="openPlaygroundSnippet(this, 'doc-snippets/blog-release-0-0-6-3-md-token_constraints')">Open In Playground</button><pre><span></span><span class="kp">argmax</span> 
    <span class="s2">"A 10 token response[WHO]"</span> 
<span class="kn">from</span> 
    <span class="s2">"openai/text-ada-001"</span> 
<span class="kp">where</span> 
    <span class="nb">len</span><span class="p">(</span><span class="n">TOKENS</span><span class="p">(</span><span class="n">WHO</span><span class="p">))</span> <span class="o">==</span> <span class="mi">10</span>
</pre></div>
<p></p>
</li>
<li><p><strong>Conditional Stopping</strong> <code class="docutils literal notranslate"><span class="pre">STOPS_AT</span></code> can now be combined with additional side conditions. This allows you to specify stopping phrases that are only enforced, once other conditions are met.</p>
<p>For example, below, we stop when the generated text hits a newline character, but only if the overall variable output is already at least 10 tokens long.</p>
<p></p><div class="highlight lmql"><button href="" onclick="openPlaygroundSnippet(this, 'doc-snippets/blog-release-0-0-6-3-md-conditional_stopping ')">Open In Playground</button><pre><span></span><span class="kp">argmax</span> 
    <span class="s2">"Hello[WHO]"</span> 
<span class="kn">from</span> 
    <span class="s2">"openai/text-ada-001"</span> 
<span class="kp">where</span> 
    <span class="nb">len</span><span class="p">(</span><span class="n">TOKENS</span><span class="p">(</span><span class="n">WHO</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">10</span> <span class="ow">and</span> <span class="n">STOPS_AT</span><span class="p">(</span><span class="n">WHO</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<p></p>
</li>
<li><p><strong>lmql.run</strong>: Improved input validation for <code class="docutils literal notranslate"><span class="pre">lmql.run</span></code> as contributed by <a href="https://twitter.com/lfegray" target="_blank">@lfegray</a>. More specifically, <code class="docutils literal notranslate"><span class="pre">lmql.run</span></code> wil now provide more helpful error messages when client logic does not specify input values for all required query parameters.</p></li>
<li><p><strong>Automatic Cache Invalidation</strong>: LMQL‚Äôs tokenizer cache at <code class="docutils literal notranslate"><span class="pre">~/.cache/lmql</span></code> is now invalidated automatically when upgrading to a new version. This should prevent issues with outdated cache files.</p></li>
</ul>
<blockquote>
<div><p>Note: Version 0.0.6.2 was skipped and yanked from pypi.org, as an invalid release was pushed accidentally.</p>
</div></blockquote>
</section>


          </div>
        </div>
    </div>
  </section>
    
<section class="section blog startpage" id="release-0.0.6.1">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h5 class="title is-2 has-text-left" style="margin-bottom: 2pt;">
            <a href="blog/release-0.0.6.1.html" class="anchor">
                LMQL v0.0.6.1
            </a>
          </h5>
          <div class="content has-text-justified">
          <div class="authors">
            <div class="is-size-5 publication-authors">
              <span class="author-block">
            <a href="mailto:hello@lmql.ai">LMQL Team</a>
        </span>
            </div>
          </div>
            <div class="date">
                Wed, May 3, 2023
            </div>
            <section id="lmql-v0-0-6-1">

<p>We released LMQL v0.0.6.1, which contains several bug fixes and improvements. The most notable changes are:</p>
<ul class="simple">
<li><p><strong>Cache Layer Bug Fixes</strong> This release contains several fixes and improvements to the recently introduced cache layer.</p></li>
<li><p><strong>Stopping Phrases</strong> Stopping phrases specified via <code class="docutils literal notranslate"><span class="pre">STOPS_BEFORE</span></code> are now passed to the OpenAI API as <code class="docutils literal notranslate"><span class="pre">"stop"</span></code> parameter, decreasing the number of tokens used for the request. If you want to disable this (e.g. to allow speculative execution), you can specify the new decoder parameter <code class="docutils literal notranslate"><span class="pre">openai_nonstop=True</span></code>.</p></li>
<li><p><strong>Asynchronous Output Writers</strong> All output writers have been refactored to use asynchronous I/O. This should simplify integration with other asynchronous frameworks, e.g. for HTTP or Websocket APIs. We also added a new chapter on <a class="reference external" href="https://docs.lmql.ai/en/latest/python/output.html">Output Streaming</a> to the documentation.</p></li>
<li><p><strong>Output Writers for HTTP endpoints, WebSockets and Server-Sent Events</strong> Based on the updated output writer interface, we added three new output writers for serving LMQL queries as HTTP endpoints, WebSockets and via Server-Sent Events (SSE). To learn more, check their relatively simple implementations in the new <a class="reference external" href="https://github.com/eth-sri/lmql/tree/main/src/lmql/output">lmql.output</a> module. We will also provide more documentation on how to use them, e.g. with <code class="docutils literal notranslate"><span class="pre">aiohttp</span></code> in the future.</p></li>
</ul>
</section>


          </div>
        </div>
    </div>
  </section>
    
<section class="section blog startpage" id="release-0.0.6">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h5 class="title is-2 has-text-left" style="margin-bottom: 2pt;">
            <a href="blog/release-0.0.6.html" class="anchor">
                Releasing the LMQL Caching Layer (v0.0.6)
            </a>
          </h5>
          <div class="content has-text-justified">
          <div class="authors">
            <div class="is-size-5 publication-authors">
              <span class="author-block">
            <a href="mailto:hello@lmql.ai">LMQL Team</a>
        </span>
            </div>
          </div>
            <div class="date">
                Mon, May 1, 2023
            </div>
            <section id="releasing-the-lmql-caching-layer-v0-0-6">

<p>Today we are releasing LMQL 0.0.6, the first version of LMQL that integrates the <em>LMQL Caching Layer</em>. The caching layer can drastically reduce token use of LLM interaction, lowering both the cost and latency of running queries. In this blog post, we provide a quick overview of the caching layer and demonstrate how it can reduce token use, latency and the number of requests needed to run queries by up to 80%. We observe improvements across a wide range of different scenarios, including <strong>template-based queries, long-form constraints and tool augmentation.</strong></p>
<p>You can experiment with LMQL in the browser-based <a class="reference external" href="http://lmql.ai/playground">Playground IDE</a> or install the latest version locally, via <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">lmql</span></code>.</p>
<section id="caching-layer">
<h2>Caching Layer<a class="headerlink" href="#caching-layer" title="Permalink to this heading">¬∂</a></h2>
<p>The caching layer is implemented as a <strong>tree-based data structure</strong> that caches all model output including logits, tokens, and metadata, allowing the runtime to more efficiently explore the token space of an LLM, even in the presence of multiple variables, constraints and tool augmentation. The cache can be considered an append-only tree, that is explored during query execution, expanding branches according to query code, constraints and speculative execution.</p>
<p>To illustrate the effect of a caching layer, we consider the following example scenarios, all of which now run in a fraction of the time and with a fraction of the tokens needed with traditional querying methods.</p>
<section id="template-based-queries">
<h3>Template-Based Queries<a class="headerlink" href="#template-based-queries" title="Permalink to this heading">¬∂</a></h3>
<p>When specifying a prompt template with multiple variables to fill in, an LLM typically needs to be invoked once per variable. For instance, consider the following template that guides an LLM in generating a list of things:</p>
<p></p><div class="highlight lmql"><button href="" onclick="openPlaygroundSnippet(this, 'doc-snippets/blog-release-0-0-6-md-list-of-things-speculative')">Open In Playground</button><pre><span></span><span class="kp">argmax</span>
    <span class="s2">"A list of things not to forget when going to the sea (not travelling): </span><span class="se">\n</span><span class="s2">"</span>
    <span class="s2">"- Sunglasses </span><span class="se">\n</span><span class="s2">"</span>
    <span class="s2">"-[THING]"</span>
    <span class="s2">"-[THING]"</span>
    <span class="s2">"-[THING]"</span>
    <span class="s2">"-[THING]"</span>
<span class="kn">from</span>
    <span class="s1">'openai/text-ada-001'</span>
<span class="kp">where</span>
    <span class="n">STOPS_AT</span><span class="p">(</span><span class="n">THING</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<p></p>
<p><strong>Without Caching:</strong> Tokens: 390, Requests: 4 | <strong>With Caching Layer:</strong> Tokens: 89 (<span style="color: green">-77%</span>), Requests: 1 (<span style="color: green">-75%</span>)</p>
<p>Here, the LLM typically needs to be invoked 4 times, once per <code class="docutils literal notranslate"><span class="pre">[THING]</span></code> variable. On each call, this incurs a token and latency cost (both with OpenAI and local models). Separate calls are needed, because our template dictates the <code class="docutils literal notranslate"><span class="pre">-</span></code> token to be inserted before each <code class="docutils literal notranslate"><span class="pre">[THING]</span></code>.</p>
<p>With the caching layer, LMQL can now invoke the LLM only once, and fill in all variables with the resulting tokens, as long as the LLM output already aligns naturally with your template. In case the LLM result of the initial invocation at some point no longer aligns with the template, LMQL will automatically re-invoke the LLM from this point on, guaranteeing an overall consistent result that is already parsed into separate <code class="docutils literal notranslate"><span class="pre">[THING]</span></code> variables.</p>
</section>
<section id="short-circuiting-long-constraints">
<h3>Short-Circuiting Long Constraints<a class="headerlink" href="#short-circuiting-long-constraints" title="Permalink to this heading">¬∂</a></h3>
<p>When you specify long constraints like <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">in</span> <span class="pre">["ABCDE",</span> <span class="pre">"FGHIJK"]</span></code>, the LMQL runtime guides the LLM to choose one of the provided options and then continues enforcing the sequence until the chosen values is fully decoded. To illustrate, consider the following query:</p>
<p></p><div class="highlight lmql"><button href="" onclick="openPlaygroundSnippet(this, 'doc-snippets/blog-release-0-0-6-md-long-form-constraints-speculative')">Open In Playground</button><pre><span></span><span class="kp">argmax</span>
    <span class="s2">"If we have the choice we choose[OPTION]"</span>
<span class="kn">from</span> 
    <span class="s2">"openai/text-ada-001"</span>
<span class="kp">where</span>
    <span class="n">OPTION</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"Option A with a whole lot of extra context"</span><span class="p">,</span> 
        <span class="s2">"Option B with context"</span><span class="p">,</span> 
        <span class="s2">"Another Option, also with a lot of additional text"</span>
    <span class="p">]</span>
</pre></div>
<div class="highlight-model-output notranslate"><div class="highlight">If we have the choice we choose <span class="variable val1"><span class="variable-name">OPTION</span> Option A with a whole lot of extra context</span></div></div><p></p>
<p><strong>Without Caching:</strong> Tokens: 123, Requests: 9 | <strong>With Caching Layer:</strong> Tokens: 25 (<span style="color: green">-80%</span>), Requests: 2 (<span style="color: green">-78%</span>)</p>
<p>Here, after the LLM has produced <code class="docutils literal notranslate"><span class="pre">"Option"</span></code> and then <code class="docutils literal notranslate"><span class="pre">"</span> <span class="pre">A"</span></code>, LMQL short-circuits further model calls and automatically completes the resulting sequence to <code class="docutils literal notranslate"><span class="pre">"Option</span> <span class="pre">A</span> <span class="pre">with</span> <span class="pre">a</span> <span class="pre">whole</span> <span class="pre">lot</span> <span class="pre">of</span> <span class="pre">extra</span> <span class="pre">context"</span></code>. This is possible because once <code class="docutils literal notranslate"><span class="pre">Option</span> <span class="pre">A</span></code> has been predicted, the remaining tokens are fully determined by the constraints.</p>
</section>
<section id="tool-augmented-queries">
<h3>Tool-Augmented Queries<a class="headerlink" href="#tool-augmented-queries" title="Permalink to this heading">¬∂</a></h3>
<p>Lastly, we consider tool augmented queries. LLM agents and tool augmentation are very powerful paradigms, that allow LLMs to incorporate external knowledge and reasoning into their predictions. However, this comes at a cost: On each tool invocation, the LLM needs to be re-invoked to continue decoding after the tool output has been inserted. This impacts both the token cost and latency of running queries, as many requests have to be send forth and back between the LLM and the tool.</p>
<p>As an example, consider the following query that augments an LLM with the ability to use a key-value storage, <a class="reference external" href="http://lmql.ai/playground?snippet=kv">also runnable in the browser-based LMQL Playground</a>.</p>
<center>
<a href="http://lmql.ai/playground?snippet=kv">
    <img src="https://user-images.githubusercontent.com/17903049/235436824-0150f73f-0ac6-4cd9-8cc9-d13343da54f0.png" alt="Key-Storage Augmented LLM implemented in LMQL" style="height:320pt;">
</a>
</center>
<p><strong>Without Caching:</strong> Tokens: 5,162, Requests: 12 | <strong>With Caching Layer:</strong> Tokens: 3,481 (<span style="color: green">-33%</span>), Requests: 8 (<span style="color: green">-33%</span>)</p>
<p>Here, whenever the LLM produces an action relating to our key-value storage, we invoke a tool that handles the storage and return the result (to <code class="docutils literal notranslate"><span class="pre">assign</span></code> and <code class="docutils literal notranslate"><span class="pre">get</span></code> stored values). The result of each tool invocation is then inserted into the LLM output, and the LLM is re-invoked to continue decoding.</p>
<p>We count 10 tool interactions which results in 12 requests if we run without caching. However, using the new caching layer, we can reduce this to 8 requests, even undercutting the number of tool interactions. This is possible because the caching layer will not abort LLM generation, if the LLM already correctly predicts the tool output.</p>
<p>This scenario demonstrates that the natural ability of LLMs to complete sequences can be leveraged to reduce the number of tool interactions, by relying on speculative execution.</p>
</section>
</section>
<section id="persisting-the-cache">
<h2>Persisting the Cache<a class="headerlink" href="#persisting-the-cache" title="Permalink to this heading">¬∂</a></h2>
<p>Of course, the in-memory cache of the LMQL runtime can also be persisted to disk, allowing you to reuse the cache tree across multiple queries, automatically reducing token cost and latency. In some cases this can even be used to reduce the number of requests to the LLM to 0, e.g. if the cache already contains the desired result.</p>
<p>To do so, you can simply specify a <code class="docutils literal notranslate"><span class="pre">cache="file.tokens"</span></code> parameter in your query code:</p>
<p></p><div class="highlight lmql"><button href="" onclick="openPlaygroundSnippet(this, 'doc-snippets/blog-release-0-0-6-md-joke-with-cache')">Open In Playground</button><pre><span></span><span class="kp">argmax</span><span class="p">(</span><span class="n">cache</span><span class="o">=</span><span class="s2">"joke.tokens"</span><span class="p">)</span>
<span class="w">   </span><span class="sd">"""A good dad joke. A indicates the punchline</span>
<span class="sd">   Q:[JOKE]</span>
<span class="sd">   A:[PUNCHLINE]"""</span>
<span class="kn">from</span>
   <span class="s2">"openai/text-davinci-003"</span>
<span class="kp">where</span>
   <span class="nb">len</span><span class="p">(</span><span class="n">JOKE</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">120</span> <span class="ow">and</span> 
   <span class="n">STOPS_AT</span><span class="p">(</span><span class="n">JOKE</span><span class="p">,</span> <span class="s2">"?"</span><span class="p">)</span> <span class="ow">and</span> 
   <span class="n">STOPS_AT</span><span class="p">(</span><span class="n">PUNCHLINE</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span> <span class="ow">and</span> 
   <span class="nb">len</span><span class="p">(</span><span class="n">PUNCHLINE</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span>
</pre></div>
<p></p>
<p>The first successful run of this query will persist the cache to <code class="docutils literal notranslate"><span class="pre">joke.tokens</span></code>. Subsequent runs will then automatically load the cache from disk, and only invoke the LLM if the cache does not contain a match. This also works for queries whose underlying LLM requests only partially overlap, as the tree-based cache data structure will automatically identify matching subtrees.</p>
<p><strong>Caching During Query Development</strong>: Persisting the cache can be particularly useful during query development, as it allows you to reuse the cache across multiple runs of the same query. A persistent cache will reduce token cost and latency of your query, even if you slightly change the query between runs.</p>
</section>
<section id="caveats-and-disabling-the-cache">
<h2>Caveats and Disabling the Cache<a class="headerlink" href="#caveats-and-disabling-the-cache" title="Permalink to this heading">¬∂</a></h2>
<p>You can disable the caching layer by specifying <code class="docutils literal notranslate"><span class="pre">cache=False</span></code> in your query code. This will cause the LMQL runtime to always invoke the LLM, and never use the cache. This is useful for debugging purposes, or if you want to ensure that the LLM is always invoked.</p>
<p>Further, as the cache currently is implemented as an append-only data structure, it will grow indefinitely. This may be problematic for long-running applications, as the cache will eventually grow to relatively large sizes. In the future, we plan to implement simple strategies to limit the cache size, such as a least-recently-used eviction policy.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">¬∂</a></h2>
<p>In this post, we introduced the new caching layer of the LMQL runtime, which allows you to reduce the token cost and latency of your queries by reusing previously generated LLM outputs. We demonstrated how the caching layer can be used to reduce the number of LLM invocations in a variety of scenarios, including long constraints, short-circuiting, and tool-augmented queries. We also showed how the cache can be persisted to disk, allowing you to reuse the cache across multiple queries.</p>
<p>To learn more about LMQL please also check out our <a class="reference external" href="https://docs.lmql.ai">documentation</a>, or join our <a class="reference external" href="https://discord.gg/2Y3Wz2Q">Discord</a> to chat with us directly. We are looking forward to hearing from you!</p>
</section>
</section>


          </div>
        </div>
    </div>
  </section>
    
<section class="section blog startpage" id="release-0.0.5">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h5 class="title is-2 has-text-left" style="margin-bottom: 2pt;">
            <a href="blog/release-0.0.5.html" class="anchor">
                LMQL Release 0.0.5
            </a>
          </h5>
          <div class="content has-text-justified">
          <div class="authors">
            <div class="is-size-5 publication-authors">
              <span class="author-block">
            <a href="mailto:hello@lmql.ai">LMQL Team</a>
        </span>
            </div>
          </div>
            <div class="date">
                Mon, Apr 17, 2023
            </div>
            <section id="lmql-release-0-0-5">

<p>Today we are releasing version 0.0.5 of LMQL. This release focuses on stability and performance improvements. For a detailed list of changes, please see below. We are particularly excited about the first community contributions that have been merged as part of this release, with many more in the works.</p>
<p><code class="docutils literal notranslate"><span class="pre">lmql==0.0.5</span></code> has been published on <a class="reference external" href="https://pypi.org/project/lmql/">PyPI</a>, based the current <code class="docutils literal notranslate"><span class="pre">main</span></code> branch of the <a class="reference external" href="https://github.com/eth-sri/lmql">GitHub repository</a>. The updated version has also been deployed to the browser-based <a class="reference external" href="http://lmql.ai/playground">lmql.ai/playground</a>.</p>
<section id="changelog">
<h2>Changelog<a class="headerlink" href="#changelog" title="Permalink to this heading">¬∂</a></h2>
<ul>
<li><p><strong>Decoder Performance</strong> The <code class="docutils literal notranslate"><span class="pre">argmax</span></code> and <code class="docutils literal notranslate"><span class="pre">sample</span></code> decoders have undergone some optimizations, allowing them to run faster. This results in a <em>20-30% speed-up</em> on common query workloads. <a class="reference external" href="https://github.com/eth-sri/lmql/pull/24">#24</a>.</p></li>
<li><p><strong>Postprocessing Semantics</strong> Internally, LMQL now allows constraints to implement postprocessing semantics. This is used to convert variable values after they have been completed, to a more normalized form in the prompt, and to a semantically meaningful data type in the context of the query code. <a class="reference external" href="https://github.com/eth-sri/lmql/pull/24">#24</a>.</p>
<p>For example, when using an <code class="docutils literal notranslate"><span class="pre">INT(&lt;var&gt;)</span></code> constraint on a generated number, the model will be restricted to only generate valid integers, and now, the resulting <code class="docutils literal notranslate"><span class="pre">NUM</span></code> value will additionally be converted to an <code class="docutils literal notranslate"><span class="pre">int</span></code> value:</p>
 <div class="highlight lmql"><button href="" onclick="openPlaygroundSnippet(this, 'doc-snippets/releases-release-0-0-5-md-postprocessing-int-value')">Open In Playground</button><pre><span></span><span class="kp">argmax</span>
 <span class="s2">"My favorite number is: [NUM]</span><span class="se">\n</span><span class="s2">"</span>
 <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">NUM</span><span class="p">),</span> <span class="n">NUM</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># &lt;class 'int'&gt; 4</span>
 <span class="s2">"Number times two is {NUM * 2}"</span>
 <span class="kn">from</span>
    <span class="s1">'openai/text-ada-001'</span>
 <span class="kp">where</span>
    <span class="n">INT</span><span class="p">(</span><span class="n">NUM</span><span class="p">)</span> </pre></div>
</li>
<li><p><strong>Core Interpreter</strong> A complete reimplementation of the LMQL core interpreter has been completed. This fixes a couple of minor issues and overall, improves reliability and performance when dealing with <em>branching</em> decoding algorithms. <a class="reference external" href="https://github.com/eth-sri/lmql/pull/24">#24</a>.</p></li>
<li><p><strong>Playground</strong> Locally and when used in-browser, the <a class="reference external" href="http://lmql.ai/playground">LMQL Playground</a> now <em>streams debugger information</em> from the LMQL interpreter incrementally. This leads to speed-ups when running in the Playground, especially with longer outputs. <a class="reference external" href="https://github.com/eth-sri/lmql/commit/27f9a8adb819f732608ef61c9aca9dca579dc536">#27f9a8ad</a>.</p></li>
<li><p><strong>Other Fixes</strong>:</p>
<ul class="simple">
<li><p>When used from within Python (as decorated function), LMQL code no longer has to be doubly-escaped, e.g. you can now write <code class="docutils literal notranslate"><span class="pre">STOPS_AT(VAR,</span> <span class="pre">"\n")</span></code> instead of <code class="docutils literal notranslate"><span class="pre">STOPS_AT(VAR,</span> <span class="pre">"\\n")</span></code></p></li>
<li><p>The LMQL inference API buffers requests that come in during startup, to avoid errors when the server is not yet ready. <a class="reference external" href="https://github.com/eth-sri/lmql/pull/15">#15</a>, thanks to <a class="reference external" href="https://github.com/chrispan">@chrispan</a>.</p></li>
<li><p>OpenAI request parallelization no longer leads to an error on Linux systems, with regards to worker processes <a class="reference external" href="https://github.com/eth-sri/lmql/issues/6">#6</a>.</p></li>
</ul>
</li>
</ul>
</section>
<section id="preview">
<h2>Preview<a class="headerlink" href="#preview" title="Permalink to this heading">¬∂</a></h2>
<p>Apart from the changes above, we are also working on a number of other features, including:</p>
<ul class="simple">
<li><p><strong>llama.cpp support</strong> as started in <a class="reference external" href="https://github.com/eth-sri/lmql/pull/18">this PR</a>, thanks to <a class="reference external" href="https://github.com/CircArgs">@CircArgs</a>.</p></li>
<li><p>Support for <strong>Type Constraints</strong>, e.g.  <code class="docutils literal notranslate"><span class="pre">type(VAR)</span> <span class="pre">is</span> <span class="pre">DataClass</span></code>, that automatically force the model to produce a value that structurally conforms to the given type. See this <a class="reference external" href="https://twitter.com/lbeurerkellner/status/1646187597901733889">Twitter thread</a> for more details.</p></li>
<li><p>Support for using <strong>Antlr parsers</strong> during query execution, to force the model to produce a value that conforms to a given grammar.</p></li>
<li><p><strong>Extending Logit Masking to OpenAI Chat Models</strong>. This will enable full support for LMQL constraints with e.g. <code class="docutils literal notranslate"><span class="pre">chatgpt</span></code> and <code class="docutils literal notranslate"><span class="pre">gpt-4</span></code> models. See <a class="reference external" href="https://github.com/eth-sri/lmql/pull/25">#25</a>, thanks to <a class="reference external" href="https://github.com/kharvd">@kharvd</a>.</p></li>
</ul>
</section>
</section>


          </div>
        </div>
    </div>
  </section>
    

<section class="section custom-footer">
  <div class="container is-max-desktop content">
    <div class="columns">
      <div class="column" style="text-align: left;">
        LMQL is a project by the <a href="https://www.sri.inf.ethz.ch/">Secure, Reliable, and Intelligent Systems Lab</a> at ETH Z√ºrich.<br/>
        <img src="static/images/institution-logos.svg"/>
      </div>
      <div class="column" style="text-align: left;">
        Site template adapted from <a href="http://nerfies.github.io/">Nerfies</a> by Keunhong Park et al. and uses <a href="https://bulma.io/">Bulma</a>.<br/>
        Last updated on Sat, Aug 19, 10:42 AM (UTC)<br/>
      </div>
    </div>
  </div>
</section>

</body>
</html>

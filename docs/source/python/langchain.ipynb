{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Integration ðŸ¦œðŸ”— "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LMQL can also be used together with the [ðŸ¦œðŸ”— LangChain](https://python.langchain.com/en/latest/index.html#) python library. Both, using langchain libraries from LMQL code and using LMQL queries as part of chains are supported."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain from LMQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first consider the case, where one may want to use LangChain modules as part of an LMQL query.\n",
    "\n",
    "In this example, we want to leverage the LangChain `Chroma` retrieval model, to enable question answering about a text document (the LMQL paper in this case).\n",
    "\n",
    "First, we need to import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbsphinx": "hidden",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup lmql path (not shown in documentation, metadata has nbshpinx: hidden)\n",
    "import sys \n",
    "sys.path.append(\"../../../src/\")\n",
    "# load and set OPENAI_API_KEY\n",
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"] = open(\"../../../api.env\").read().split(\"\\n\")[1].split(\": \")[1].strip()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmql\n",
    "import asyncio\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load and embed the text of the relevant document (`lmql.txt` in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text of LMQL paper\n",
    "with open(\"lmql.txt\") as f:\n",
    "    contents = f.read()\n",
    "texts = []\n",
    "for i in range(0, len(contents), 120):\n",
    "    texts.append(contents[i:i+120])\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_texts(texts, embeddings, \n",
    "    metadatas=[{\"text\": t} for t in texts], persist_directory=\"lmql-index\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then construct a chatbot function, using a simple LMQL query, that first prompts the user for a question via `await input(...)`, retrieves relevant text paragraphs using LangChain and then produces an answer using `openai/gpt-3.5-turbo` (ChatGPT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is LMQL?\n",
      "\u001b[32mReading relevant pages...\u001b[0m\n",
      "LMQL stands for Model Query Language. It is a high-level language that combines declarative SQL-like elements with an imperative syntax.\n",
      "Question: How does it work?\n",
      "\u001b[32mReading relevant pages...\u001b[0m\n",
      "LMQL works by executing all calls in lockstep, which allows for batching calls to the underlying model. This ensures that the queries are executed non-deterministically."
     ]
    }
   ],
   "source": [
    "import termcolor\n",
    "\n",
    "@lmql.query\n",
    "async def chatbot():\n",
    "        '''lmql\n",
    "sample(temperature=0.2, max_len=2048, openai_chunksize=2048)\n",
    "    \"\"\"You are a chatbot that helps users answer questions.\n",
    "    You are first provided with the question and relevant information.\"\"\"\n",
    "    while True:\n",
    "        q = await input(\"\\nQuestion: \")\n",
    "        if q == \"exit\": break\n",
    "        \"Question: {q}\\n\"\n",
    "        print(termcolor.colored(\"Reading relevant pages...\", \"green\"))\n",
    "        results = set([d.page_content for d in docsearch.similarity_search(q, 4)])\n",
    "        information = \"\\n\\n\".join([\"...\" + r + \"...\" for r in list(results)])\n",
    "        \"\\nRelevant Information: {information}\\n\"\n",
    "        \"Your response based on relevant information:[RESPONSE]\"\n",
    "from\n",
    "    \"openai/gpt-3.5-turbo\"\n",
    "        '''\n",
    "\n",
    "await chatbot(output_writer=lmql.stream(variable=\"RESPONSE\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the query, inline LMQL code appearing in a Python script can access the outer scope containing e.g. the `docsearch` variable, and access any relevant utility functions and object defined in Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LMQL from LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to using langchain utilities in LMQL query code, LMQL queries can also seamlessly be integrated as a `langchain` `Chain` component. \n",
    "\n",
    "For this consider, the sequential prompting example from the `langchain` documentation, where we first prompt the language model to propose a company name for a given product, and then ask it for a catchphrase.\n",
    "\n",
    "To get started, we first import the relevant langchain components, as well as LMQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (ChatPromptTemplate,HumanMessagePromptTemplate)\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import lmql"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our chain has two stages: (1) Asking the model for a company name, and (2) asking the model for a catchphrase. For the sake of this example, we will implement (1) in with a langchain prompt and (2) with an LMQL query. \n",
    "\n",
    "First, we define the langchain prompt for the company name and instantiate the resulting `LLMChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the LM to be used by langchain\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            template=\"What is a good name for a company that makes {product}?\",\n",
    "            input_variables=[\"product\"],\n",
    "        )\n",
    "    )\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "chat = ChatOpenAI(temperature=0.9)\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt_template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can already be executed to produce a company name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rainbow Socks Co.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"colorful socks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define prompt (2) in LMQL, i.e. the LMQL query generating the catchphrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lmql.query\n",
    "async def write_catch_phrase(company_name: str):\n",
    "    '''\n",
    "    argmax \"Write a catchphrase for the following company: {company_name}. [catchphrase]\" from \"chatgpt\"\n",
    "    '''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can run this part in isolation, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"Step up your style with Socks Inc.\"'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(await write_catch_phrase(\"Socks Inc\"))[0].variables[\"catchphrase\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To chain the two prompts together, we can use a `SimpleSequentialChain` from `langchain`. To make an LMQL query compatible for use with `langchain`, just call `.aschain()` on it, before passing it to the `SimpleSequentialChain` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, write_catch_phrase.aschain()], verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the overall chain, relying both on LMQL and langchain components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the chain specifying only the input variable for the first chain.\n",
    "catchphrase = overall_chain.run(\"colorful socks\")\n",
    "print(catchphrase)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "> Entering new SimpleSequentialChain chain...\n",
    "RainbowSocks Co.\n",
    " \"Step into a world of color with RainbowSocks Co.!\"\n",
    "\n",
    "> Finished chain.\n",
    " \"Step into a world of color with RainbowSocks Co.!\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the full chain currently only works when run outside a Jupyter environment (in a script), due to the way langchain and LMQL rely on async vs. sequential calls. With further improvements to the `langchain` async API, this limitation can be expected to be removed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

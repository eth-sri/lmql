import{_ as e,o as a,c as o,Q as t}from"./chunks/framework.4636910e.js";const g=JSON.parse('{"title":"Replicate","description":"","frontmatter":{},"headers":[],"relativePath":"docs/models/replicate.md","filePath":"docs/models/replicate.md"}'),s={name:"docs/models/replicate.md"},n=t(`<h1 id="replicate" tabindex="-1">Replicate <a class="header-anchor" href="#replicate" aria-label="Permalink to &quot;Replicate&quot;">â€‹</a></h1><p><a href="https://replicate.com/" target="_blank" rel="noreferrer">Replicate</a> is a commercial service that can run models uploaded to them in Docker containers, in the format constructed by their <a href="https://github.com/replicate/cog" target="_blank" rel="noreferrer">Cog</a> build tool. Several of these have already been uploaded as public models.</p><p>For public models, Replicate only charges for actual GPU time used; for private models, they also charge for startup and idle time. Several models wrapped for LMQL/LMTP use have already been uploaded publicly, and this chapter documents how to build, operate and deploy more.</p><h2 id="running-a-ðŸ¤—-transformers-model-on-replicate" tabindex="-1">Running A ðŸ¤— Transformers Model On Replicate <a class="header-anchor" href="#running-a-ðŸ¤—-transformers-model-on-replicate" aria-label="Permalink to &quot;Running A ðŸ¤— Transformers Model On Replicate&quot;">â€‹</a></h2><p>To run a <a href="./hf.html">ðŸ¤— Transformers</a> model on Replicate, you need to:</p><ol><li><p>Export the environment variable <code>REPLICATE_API_TOKEN</code> with the credential to use to authenticate the request.</p></li><li><p>Set the <code>transport=</code> argument to your model to <code>replicate:ORG/MODEL</code>, matching the name with which the model was uploaded.</p></li><li><p>Set the <code>tokenizer=</code> argument to your model to a huggingface transformers name from which correct configuration for the tokenizer in use can be downloaded.</p></li></ol><p>For example:</p><div class="language-lmql vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">lmql</span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">argmax</span>
    <span class="hljs-string">&quot;&quot;&quot;Review: We had a great stay. Hiking in the mountains was fabulous and the food is really good.\\n
    Q: What is the underlying sentiment of this review and why?\\n
    A:<span class="hljs-placeholder">[ANALYSIS]</span>\\n
    Q: Summarizing the above analysis in a single word -- of the options &quot;positive&quot;, &quot;negative&quot;, and &quot;neutral&quot; -- how is the review best described?\\n
    A:<span class="hljs-placeholder">[CLASSIFICATION]</span>&quot;&quot;&quot;</span>
<span class="hljs-keyword">from</span> lmql.model(
    <span class="hljs-comment"># model name is not actually used: endpoint completely overrides model selection</span>
    <span class="hljs-string">&quot;meta-llama/Llama-2-13b-chat-hf&quot;</span>,
    <span class="hljs-comment"># in this case, uses model from https://replicate.com/charles-dyfis-net/llama-2-13b-hf--lmtp-8bit</span>
    endpoint=<span class="hljs-string">&quot;replicate:charles-dyfis-net/llama-2-13b-hf--lmtp-8bit&quot;</span>,
    <span class="hljs-comment"># choosing a model with the same tokenizer as meta-llama/Llama-2-13b-hf but ungated in huggingface</span>
    tokenizer=<span class="hljs-string">&quot;AyyYOO/Luna-AI-Llama2-Uncensored-FP16-sharded&quot;</span>,
)
<span class="hljs-keyword">where</span> STOPS_AT(ANALYSIS, <span class="hljs-string">&quot;\\n&quot;</span>) <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(TOKENS(ANALYSIS)) &lt; <span class="hljs-number">200</span>
<span class="hljs-keyword">distribution</span> CLASSIFICATION <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot; positive&quot;</span>, <span class="hljs-string">&quot; negative&quot;</span>, <span class="hljs-string">&quot; neutral&quot;</span>]
</span></code></pre></div><h2 id="uploading-a-ðŸ¤—-model-to-replicate" tabindex="-1">Uploading A ðŸ¤— Model To Replicate <a class="header-anchor" href="#uploading-a-ðŸ¤—-model-to-replicate" aria-label="Permalink to &quot;Uploading A ðŸ¤— Model To Replicate&quot;">â€‹</a></h2><p>You can also upload and deploy your own LMQL models to Replicate. To do so, first install <a href="https://github.com/replicate/cog" target="_blank" rel="noreferrer">Cog</a>. In addition to that, LMQL provides scripts that largely automate the process of building and uploading models (see the <code>scripts/replicate-build</code> section of the LMQL source distribution).</p><ol><li><p>Create a corresponding model on the <a href="https://replicate.com/" target="_blank" rel="noreferrer">Replicate</a> website.</p></li><li><p>Copy <code>config.toml.example</code> to <code>config.toml</code>, and customize it.</p><p>Change <code>dest_prefix</code> to replace <code>YOURACCOUNT</code> with the name of the actual Replicate account to which you will be uploading models.</p><p>For each model you wish to build and upload, your config file should have a <code>[models.MODELNAME]</code> section. Make sure MODELNAME reflects the name of the model as create in your Replicate account.</p><p><code>huggingface.repo</code> should reflect the Hugging Face model name you wish to wrap. If you want to pin a version, also set <code>huggingface.version</code>.</p><p>The <code>config</code> section may be used to set any values you want to pass in the <code>model_args</code> dictionary.</p></li><li><p>Run the <code>./build</code> script, with your current working directory being <code>scripts/replicate-build</code>.</p><p>This will create a <code>work/</code> subdirectory for each model defined in your configuration file.</p></li><li><p>In the <code>work/MODELNAME</code> directory, run the generated <code>./push</code> script to build and upload your model, or <code>cog predict</code> to test your model locally.</p></li></ol>`,11),l=[n];function r(i,c,d,p,h,u){return a(),o("div",null,l)}const f=e(s,[["render",r]]);export{g as __pageData,f as default};

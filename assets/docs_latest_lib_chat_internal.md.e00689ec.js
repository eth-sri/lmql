import{_ as s,o as a,c as n,Q as e}from"./chunks/framework.4636910e.js";const m=JSON.parse('{"title":"Internal Reasoning","description":"","frontmatter":{"order":3},"headers":[],"relativePath":"docs/latest/lib/chat/internal.md","filePath":"docs/latest/lib/chat/internal.md"}'),t={name:"docs/latest/lib/chat/internal.md"},o=e(`<h1 id="internal-reasoning" tabindex="-1">Internal Reasoning <a class="header-anchor" href="#internal-reasoning" aria-label="Permalink to &quot;Internal Reasoning&quot;">â€‹</a></h1><p>While user-facing question-answering is the main goal of LLM-based chatbots, performance can be considerably improved by implementing internal reasoning and reflection mechanisms. In this chapter, we will discuss the implementation of such mechanisms in LMQL Chat.</p><figure align="center" style="width:100%;margin:auto;" alt="Screenshot of the model dropdown in the playground"><img style="min-height:100pt;" src="https://github.com/eth-sri/lmql/assets/17903049/cb609b5c-8984-414a-a3b6-b3fa6f8ab6bb" alt="Screenshot of the model dropdown in the playground"><figcaption>A chatbot that relies on internal reasoning.</figcaption></figure><p>Building on the simple chat application implemented in <a href="./overview.html"></a>, we extend the chat loop as follows:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="hljs"><code><span class="line"><span class="hljs-keyword">from</span> lmql.lib.chat <span class="hljs-keyword">import</span> message

<span class="hljs-keyword">argmax</span> 
    <span class="hljs-string">&quot;<span class="hljs-subst">{:system}</span> You are a marketing chatbot for the language \\
    model query language (LMQL). Before answering provide \\
    some internal reasoning to reflect. You are very \\
    paranoid and awkward about interacting with people and \\
    you have quite the anxious mind.&quot;</span>
    
    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
        <span class="hljs-string">&quot;<span class="hljs-subst">{:user}</span> <span class="hljs-subst">{<span class="hljs-keyword">await</span> <span class="hljs-built_in">input</span>()}</span>&quot;</span>
        <span class="hljs-string">&quot;<span class="hljs-subst">{:assistant}</span> Internal Reasoning:<span class="hljs-placeholder">[REASONING]</span>&quot;</span> \\
            <span class="hljs-keyword">where</span> STOPS_AT(REASONING, <span class="hljs-string">&quot;\\n&quot;</span>) <span class="hljs-keyword">and</span> \\
                  STOPS_BEFORE(REASONING, <span class="hljs-string">&quot;External Answer:&quot;</span>)
        <span class="hljs-string">&quot;<span class="hljs-subst">{:assistant}</span> External Answer: <span class="hljs-placeholder">[@message ANSWER]</span>&quot;</span>
<span class="hljs-keyword">from</span>
   <span class="hljs-string">&quot;chatgpt&quot;</span>
</span></code></pre></div><p>To implement internal reasoning, we adjust our query program in three ways:</p><ol><li><p>We adapt the <code>{:system}</code> prompt to include additional instructions that make sure the underlying LLM is instructed to produce internal reasoning output.</p></li><li><p>We add a new <code>{:assistant}</code> prompt statement that is used to generate internal reasoning. We add constraints on stopping behavior, such that internal and external reasoning are separated into variables <code>REASONING</code> and <code>ANSWER</code>.</p></li><li><p>We make sure <strong>not to</strong> annotate <code>REASONING</code> as <code>@message</code>, which hides it from the user.</p></li></ol><p>If we run this query program as a chat application, we can observe external and internal output as shown in the screenshot above. As specified by the system prompt, the chabot now indeed exhibits anxious and slighlty paranoid internal reasoning.</p>`,8),i=[o];function l(r,p,c,h,d,u){return a(),n("div",null,i)}const b=s(t,[["render",l]]);export{m as __pageData,b as default};

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMQL in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# setup lmql path (not shown in documentation, metadata has nbshpinx: hidden)\n",
    "import sys \n",
    "sys.path.append(\"../../../src/\")\n",
    "# load and set OPENAI_API_KEY\n",
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"] = open(\"../../../api.env\").read().split(\"\\n\")[1].split(\": \")[1].strip()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# disable logit bias logging\n",
    "import lmql.runtime.bopenai.batched_openai as batched_openai\n",
    "batched_openai.set_logit_bias_logging(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LMQL can be used in the Playground or as a standalone language. However, it is also possible to use LMQL as a Python library. This allows you to integrate LMQL code into your own Python projects. This chapter provides a brief overview of the LMQL Python Integration."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LMQL as Decorated Python Functions\n",
    "\n",
    "To define and run LMQL queries within Python, you can simply decorate a Python function with the `lmql.query` decorator, and provide the query code as a multi-line string. The decorated function will then automatically be compiled into a LMQL query, and can be called with the same arguments as the original Python function. The return value of the decorated function will be the result of the LMQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello everyone'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lmql\n",
    "\n",
    "@lmql.query\n",
    "async def hello(): \n",
    "    '''lmql\n",
    "    argmax \n",
    "        \"Hello[WHO]\" \n",
    "    from \n",
    "        \"openai/text-ada-001\" \n",
    "    where \n",
    "        len(WHO) < 10\n",
    "    '''\n",
    "\n",
    "(await hello())[0].prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Asynchronous API** LMQL implements a fully asynchronous API. This means that all LMQL queries are executed asynchronously, and return a `Future` object. To execute a query, you therefore have to use the `await` keyword as shown above. By relying on the `asyncio` library, LMQL also supports the execution of arbitrary many queries in parallel, using `asyncio.gather`, where on the backend the runtime will automatically optimize and schedule the queries to maximize throughput (e.g. auto batching across multiple queries).\n",
    "\n",
    "**Result Type** An LMQL query that is executed in a Python context, always returns a list of `LMQLResult` objects, where the number of elements is determined by the underlying decoding algorithm (e.g. `n` with `beam(n=n)`). The `LMQLResult` object is a simple dataclass with the following fields:\n",
    "\n",
    "```python\n",
    "class LMQLResult:\n",
    "    # full prompt with all variables substituted\n",
    "    prompt: str \n",
    "    # a dictionary of all assigned template variable values\n",
    "    variables: Dict[str, str] \n",
    "```\n",
    "\n",
    "From such a result object, you can easily extract the prompt and the assigned variable values, no need for further processing. For `distribution` variables (cf. [Distribution Clause](../language/overview.md#extracting-more-information-with-distributions)), the `variables` dictionary will additionally contain `log P(VAR)` and `P(VAR)` entries, allowing users to easily extract the probability of the assigned value.\n",
    "\n",
    "`@lmql.query` functions like `hello()` also conform to the interface required by the `langchain` library, as detailed in the [LangChain Integration](./langchain.ipynb#Using-LMQL-from-LangChain) chapter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program Context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@lmql.query` annotated functions are fully integrated into the provided program context, meaning you can provide function arguments and even access variables from the containing scope during query execution. This allows you to use LMQL as a fully integrated part of your Python program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Greet the Python interpreter and the World: \\n\\nHello, I am here!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lmql\n",
    "\n",
    "someone = \"the Python interpreter\"\n",
    "\n",
    "@lmql.query\n",
    "async def greet(someone_else):\n",
    "    '''\n",
    "    argmax \"Greet {someone} and {someone_else}: [WHO]\" from \"openai/text-ada-001\" where len(WHO) < 20\n",
    "    '''\n",
    "\n",
    "(await greet(\"the World\"))[0].prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also allows you to call utility functions, that query external information systems like Wikipedia, and incorporate the obtained information dynamically in your prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greet Earth (Earth is the third planet from the Sun and the only place known in the universe where life has originated and found habitability): \n",
      "Hello Earth! It's wonderful to meet you. You are a unique and special place in the universe, and we are so lucky to call you home.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lmql\n",
    "import aiohttp\n",
    "\n",
    "async def look_up(term):\n",
    "    # looks up term on wikipedia\n",
    "    url = f\"https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro&explaintext&redirects=1&titles={term}&origin=*\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as response:\n",
    "            # get the first sentence on first page\n",
    "            page = (await response.json())[\"query\"][\"pages\"]\n",
    "            return list(page.values())[0][\"extract\"].split(\".\")[0]\n",
    "\n",
    "@lmql.query\n",
    "async def greet(term):\n",
    "    '''\n",
    "    argmax \n",
    "        \"\"\"Greet {term} ({await look_up(term)}): \n",
    "        Hello[WHO]\n",
    "        \"\"\"\n",
    "    from \n",
    "        \"openai/text-davinci-003\" \n",
    "    where \n",
    "        STOPS_AT(WHO, \"\\n\")\n",
    "    '''\n",
    "\n",
    "print((await greet(\"Earth\"))[0].prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This capability of calling arbitrary program logic during query execution, enables powerful use cases like the integration of retrieval, as discussed in the [LangChain Integration](./langchain.ipynb) chapter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntax Highlighting\n",
    "\n",
    "LMQL also provides a syntax highlighter for Visual Studio Code. Both `.lmql` files as well as `.py` files with `@lmql.query` decorated functions will be highlighted automatically. The syntax highlighter is available as a [VSCode extension](https://marketplace.visualstudio.com/items?itemName=lmql-team.lmql).\n",
    "\n",
    "To enable syntax highlighting in Python files, make sure to lead the LMQL code in an `@lmql.query` function with the `lmql` code block type:\n",
    "\n",
    "<img width=\"528\" alt=\"image\" src=\"https://user-images.githubusercontent.com/17903049/230897738-a676cd6b-bec2-4403-8628-1c10d3da263c.png\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
